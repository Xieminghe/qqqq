1. native_dropout
  - native_dropout


  - native_dropout_cpu(CPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Dropout.cpp:107:native_dropout_cpu(const Tensor& input, double p, c10::optional<bool> train) {

  - native_dropout_cuda(CUDA)
  - native_dropout_nested(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:478:std::tuple<Tensor,Tensor> native_dropout_nested(const Tensor& input, double p, c10::optional<bool> train) {

2. abs
  - abs
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:460:Tensor abs(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:549:Tensor abs(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:21:Tensor NestedTensor_abs(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:580:abs(const complex<_Tp>& __c)

  - abs(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:460:Tensor abs(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:549:Tensor abs(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:21:Tensor NestedTensor_abs(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:580:abs(const complex<_Tp>& __c)

  - abs_sparse(SparseCPU, SparseCUDA)
  - abs_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
  - NestedTensor_abs(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:21:Tensor NestedTensor_abs(const Tensor& self) {

3. real
  - real
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:581:Tensor real(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:819:std::pair<Tensor, Tensor> complex_to_real(const Tensor& inp) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:362:Tensor isreal(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:544:real(const complex<_Tp>& __c)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:552:real(_Tp __re)

4. imag
  - imag
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:602:Tensor imag(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:562:imag(const complex<_Tp>& __c)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:570:imag(_Tp)

5. conj
  - conj
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:649:Tensor resolve_conj(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:655:Tensor _conj(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:662:Tensor conj(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TypeProperties.cpp:55:bool is_conj(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:689:conj(const complex<_Tp>& __c)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:697:conj(_Tp __re)

6. conj_physical
  - conj_physical
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:622:Tensor _conj_physical(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:629:Tensor conj_physical(const Tensor& self) {

7. acos
  - acos
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:492:Tensor arccos(const Tensor& self) { return self.acos(); }
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:1071:acos(const complex<_Tp>& __x)

8. add.Tensor
  - add
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/Unfold2d.cpp:16:static inline void cadd(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp:442:mask_scatter_add(const scalar_t *src, scalar_t* base_addr,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:118:index_select_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:201:index_select_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:381:index_select_add(const Tensor &select_indices,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:510:index_select_scale_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:570:index_select_scale_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:761:index_select_scale_add(const Tensor &select_indices,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp:433:Tensor quantized_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:339:Tensor index_add(const Tensor& self, Dimname dim, const Tensor& index, const Tensor& source, const Scalar &alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:384:Tensor scatter_add(const Tensor& self, Dimname dim, const Tensor& index, const Tensor& source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:28:Tensor mkldnn_add(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:101:Tensor mkldnn_add(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1117:at::Tensor PackedConvWeightsOnednn<kSpatialDim>::apply_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:138:Tensor qnnpack_add(Tensor qa, Tensor qb, double scale, int64_t zero_point) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:283:Tensor xnnp_add(Tensor qa, Tensor qb, double scale, int64_t zero_point) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:376:Tensor qadd(Tensor qa, Tensor qb, double scale, int64_t zero_point) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:486:static Tensor quantized_add(Tensor qa, Tensor qb, double scale, int64_t zero_point){
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cudnn/BinaryOps.cpp:90:Tensor add(Tensor qa, Tensor qb, double output_scale, int64_t output_zero_point) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1161:Tensor add(const Tensor& self, const Scalar& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/benchmarks/tensor_add.cpp:5:static void tensor_add(benchmark::State& state) {

  - add_sparse(SparseCPU, SparseCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:416:Tensor add_sparse(const Tensor& self, const Tensor& other, const Scalar& alpha) {

  - add_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:781:Tensor add_sparse_csr(

  - mkldnn_add(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:28:Tensor mkldnn_add(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:101:Tensor mkldnn_add(const Tensor& self, const Tensor& other, const Scalar& alpha) {

  - add_zerotensor(ZeroTensor)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1076:Tensor add_zerotensor(const Tensor& self, const Tensor& other, const Scalar& alpha) {

  - NestedTensor_add_Tensor(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp:171:Tensor NestedTensor_add_Tensor(

9. add.Scalar
  - add
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/Unfold2d.cpp:16:static inline void cadd(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp:442:mask_scatter_add(const scalar_t *src, scalar_t* base_addr,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:118:index_select_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:201:index_select_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:381:index_select_add(const Tensor &select_indices,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:510:index_select_scale_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:570:index_select_scale_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:761:index_select_scale_add(const Tensor &select_indices,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp:433:Tensor quantized_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:339:Tensor index_add(const Tensor& self, Dimname dim, const Tensor& index, const Tensor& source, const Scalar &alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:384:Tensor scatter_add(const Tensor& self, Dimname dim, const Tensor& index, const Tensor& source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:28:Tensor mkldnn_add(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:101:Tensor mkldnn_add(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1117:at::Tensor PackedConvWeightsOnednn<kSpatialDim>::apply_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:138:Tensor qnnpack_add(Tensor qa, Tensor qb, double scale, int64_t zero_point) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:283:Tensor xnnp_add(Tensor qa, Tensor qb, double scale, int64_t zero_point) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:376:Tensor qadd(Tensor qa, Tensor qb, double scale, int64_t zero_point) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:486:static Tensor quantized_add(Tensor qa, Tensor qb, double scale, int64_t zero_point){
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cudnn/BinaryOps.cpp:90:Tensor add(Tensor qa, Tensor qb, double output_scale, int64_t output_zero_point) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1161:Tensor add(const Tensor& self, const Scalar& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/benchmarks/tensor_add.cpp:5:static void tensor_add(benchmark::State& state) {

  - add(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/Unfold2d.cpp:16:static inline void cadd(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp:442:mask_scatter_add(const scalar_t *src, scalar_t* base_addr,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:118:index_select_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:201:index_select_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:381:index_select_add(const Tensor &select_indices,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:510:index_select_scale_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:570:index_select_scale_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:761:index_select_scale_add(const Tensor &select_indices,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp:433:Tensor quantized_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:339:Tensor index_add(const Tensor& self, Dimname dim, const Tensor& index, const Tensor& source, const Scalar &alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:384:Tensor scatter_add(const Tensor& self, Dimname dim, const Tensor& index, const Tensor& source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:28:Tensor mkldnn_add(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:101:Tensor mkldnn_add(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1117:at::Tensor PackedConvWeightsOnednn<kSpatialDim>::apply_add(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:138:Tensor qnnpack_add(Tensor qa, Tensor qb, double scale, int64_t zero_point) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:283:Tensor xnnp_add(Tensor qa, Tensor qb, double scale, int64_t zero_point) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:376:Tensor qadd(Tensor qa, Tensor qb, double scale, int64_t zero_point) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:486:static Tensor quantized_add(Tensor qa, Tensor qb, double scale, int64_t zero_point){
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cudnn/BinaryOps.cpp:90:Tensor add(Tensor qa, Tensor qb, double output_scale, int64_t output_zero_point) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1161:Tensor add(const Tensor& self, const Scalar& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/benchmarks/tensor_add.cpp:5:static void tensor_add(benchmark::State& state) {

10. arange.start_step
  - arange
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:65:bool solve_arange(const Tensor& input, int64_t& start, int64_t& end, int64_t& step) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:134:Tensor arange(const Scalar& end,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:142:Tensor arange(const Scalar& start, const Scalar& end,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:151:Tensor arange(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:186:Tensor _dim_arange(const Tensor& like, int64_t dim) {

  - arange(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:65:bool solve_arange(const Tensor& input, int64_t& start, int64_t& end, int64_t& step) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:134:Tensor arange(const Scalar& end,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:142:Tensor arange(const Scalar& start, const Scalar& end,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:151:Tensor arange(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:186:Tensor _dim_arange(const Tensor& like, int64_t dim) {

11. argmax
  - argmax
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:773:static Tensor argmax(const Tensor& /*self*/, Dimname /*dim*/, bool /*keepdim*/) {

12. argmin
  - argmin
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:776:static Tensor argmin(const Tensor& /*self*/, Dimname /*dim*/, bool /*keepdim*/) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:212:static void check_argmax_argmin(

13. acosh
  - acosh
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:760:Tensor arccosh(const Tensor& self) { return at::acosh(self); }
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:938:acosh(const complex<_Tp>& __x)

14. asinh
  - asinh
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:765:Tensor arcsinh(const Tensor& self) { return self.asinh(); }
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:909:asinh(const complex<_Tp>& __x)

  - asinh_sparse(SparseCPU, SparseCUDA)
  - asinh_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
15. atanh
  - atanh
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:770:Tensor arctanh(const Tensor& self) { return self.atanh(); }
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:972:atanh(const complex<_Tp>& __x)

  - atanh_sparse(SparseCPU, SparseCUDA)
  - atanh_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
16. as_strided
  - as_strided


  - as_strided_tensorimpl(ZeroTensor, CPU, CUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1140:Tensor as_strided_tensorimpl(const Tensor& self, IntArrayRef size, IntArrayRef stride, optional<int64_t> storage_offset_) {

  - as_strided_tensorimpl_meta_symint(Meta)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1167:Tensor as_strided_tensorimpl_meta_symint(const Tensor& self, SymIntArrayRef sym_size, SymIntArrayRef sym_stride, optional<c10::SymInt> sym_storage_offset_) {

  - as_strided_tensorimpl_mps(MPS)
  - as_strided_qtensorimpl(QuantizedCPU, QuantizedCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1181:Tensor as_strided_qtensorimpl(const Tensor& self, IntArrayRef size, IntArrayRef stride, optional<int64_t> storage_offset_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1198:static Tensor as_strided_qtensorimpl(const Tensor& self, IntArrayRef size, IntArrayRef stride, optional<int64_t> storage_offset_,

17. asin
  - asin
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:533:Tensor arcsin(const Tensor& self) { return self.asin(); }
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:1061:asin(const complex<_Tp>& __x)

  - asin_sparse(SparseCPU, SparseCUDA)
  - asin_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
18. atan
  - atan
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:538:Tensor arctan(const Tensor& self) { return self.atan(); }
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:1108:atan(const complex<_Tp>& __x)

  - atan_sparse(SparseCPU, SparseCUDA)
  - atan_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
19. bitwise_not
  - bitwise_not
20. logical_not
  - logical_not
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:860:Tensor logical_not(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:53:Tensor NestedTensor_logical_not(const Tensor& self) {

  - logical_not(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:860:Tensor logical_not(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:53:Tensor NestedTensor_logical_not(const Tensor& self) {

  - NestedTensor_logical_not(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:53:Tensor NestedTensor_logical_not(const Tensor& self) {

21. logical_and
  - logical_and
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1483:Tensor logical_and(const Tensor& self, const Tensor& other) { return comparison_op(self, other, static_cast<OutFunc>(at::logical_and_out)); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1486:static Tensor logical_and(const Tensor& self, const Scalar& other) { return comparison_op(self, other, static_cast<OutFunc>(at::logical_and_out)); }

  - logical_and(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1483:Tensor logical_and(const Tensor& self, const Tensor& other) { return comparison_op(self, other, static_cast<OutFunc>(at::logical_and_out)); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1486:static Tensor logical_and(const Tensor& self, const Scalar& other) { return comparison_op(self, other, static_cast<OutFunc>(at::logical_and_out)); }

22. logical_or
  - logical_or
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1490:Tensor logical_or(const Tensor& self, const Tensor& other) { return comparison_op(self, other, static_cast<OutFunc>(at::logical_or_out)); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1493:static Tensor logical_or(const Tensor& self, const Scalar& other) { return comparison_op(self, other, static_cast<OutFunc>(at::logical_or_out)); }

  - logical_or(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1490:Tensor logical_or(const Tensor& self, const Tensor& other) { return comparison_op(self, other, static_cast<OutFunc>(at::logical_or_out)); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1493:static Tensor logical_or(const Tensor& self, const Scalar& other) { return comparison_op(self, other, static_cast<OutFunc>(at::logical_or_out)); }

23. bmm
  - bmm
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:264:void common_checks_baddbmm_bmm(Meta& meta, const Tensor& batch1, const Tensor& batch2, const Scalar& beta, const Scalar& alpha, bool is_bmm, const c10::optional<Tensor>& self_baddbmm = nullopt) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1518:Tensor addbmm(const Tensor& self, const Tensor& batch1, const Tensor& batch2, const Scalar& beta, const Scalar& alpha) {

  - bmm_sparse_cpu(SparseCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1899:Tensor bmm_sparse_cpu(const SparseTensor& self, const Tensor& mat2) {

  - bmm_sparse_cuda(SparseCUDA)
  - bmm_nested(NestedTensorCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMatmul.cpp:22:Tensor bmm_nested(const Tensor& self, const Tensor& mat2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMatmul.cpp:82:static Tensor matmul_with_bmm_nested(const Tensor& self, const Tensor& mat2) {

  - bmm_nested_cuda(NestedTensorCUDA)
24. cat
  - cat
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Concat.cpp:270:Tensor cat(const at::ITensorListRef& tensors, const int64_t in_dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:632:Tensor cat(TensorList tensors, Dimname dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:642:Tensor concat(TensorList tensors, Dimname dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:650:Tensor concat(TensorList tensors, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/RNN.cpp:639:Tensor hidden_concat(at::ArrayRef<Tensor> hiddens) { return at::cat(hiddens, 0); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/RNN.cpp:640:tpair_of<Tensor> hidden_concat(at::ArrayRef<tpair_of<Tensor>> hiddens) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/RNN.cpp:1126:std::tuple<io_type, Tensor> _rnn_impl_with_concat(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:126:Tensor qcat(

  - cat_sparse(SparseCPU, SparseCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:819:Tensor cat_sparse(const ITensorListRef& tensors, int64_t dim) {

  - cat_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:160:Tensor cat_quantized_cpu(const ITensorListRef& qxs, int64_t dim) {

25. ceil
  - ceil


  - ceil_sparse(SparseCPU, SparseCUDA)
  - ceil_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
26. clamp
  - clamp
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:12:Tensor _clamp(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:69:Tensor clamp(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qclamp.cpp:34:Tensor qnnpack_clamp(Tensor input, const Scalar& min, const Scalar& max) {

  - clamp_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qclamp.cpp:129:Tensor clamp_quantized_cpu(

27. constant_pad_nd
  - constant_pad_nd
   - /home/haozhe/code/pytorch/aten/src/ATen/native/PadNd.cpp:29:Tensor constant_pad_nd(const Tensor& self, IntArrayRef pad, const Scalar& value) {

  - constant_pad_nd(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/PadNd.cpp:29:Tensor constant_pad_nd(const Tensor& self, IntArrayRef pad, const Scalar& value) {

  - constant_pad_nd_mps(MPS)
28. convolution
  - convolution
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:33:at::Tensor miopen_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:94:at::Tensor miopen_depthwise_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:794:Tensor miopen_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:895:Tensor miopen_depthwise_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NNPACK.cpp:24:at::Tensor _nnpack_spatial_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NNPACK.cpp:137:Tensor _nnpack_spatial_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Convolution.cpp:766:Tensor convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Convolution.cpp:791:Tensor quantized_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:827:at::Tensor complex_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1162:at::Tensor convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1452:at::Tensor _convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1692:at::Tensor _convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvShared.cpp:213:Tensor cudnn_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvPlaceholders.cpp:27:at::Tensor cudnn_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:24:Tensor mkldnn_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:259:static Tensor _mkldnn_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:327:Tensor mkldnn_convolution(

  - convolution(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:33:at::Tensor miopen_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:94:at::Tensor miopen_depthwise_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:794:Tensor miopen_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:895:Tensor miopen_depthwise_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NNPACK.cpp:24:at::Tensor _nnpack_spatial_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NNPACK.cpp:137:Tensor _nnpack_spatial_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Convolution.cpp:766:Tensor convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Convolution.cpp:791:Tensor quantized_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:827:at::Tensor complex_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1162:at::Tensor convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1452:at::Tensor _convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1692:at::Tensor _convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvShared.cpp:213:Tensor cudnn_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvPlaceholders.cpp:27:at::Tensor cudnn_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:24:Tensor mkldnn_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:259:static Tensor _mkldnn_convolution(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:327:Tensor mkldnn_convolution(

29. convolution_backward
  - convolution_backward
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:59:std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:115:std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_depthwise_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:1420:std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:1441:std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_depthwise_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1977:std::tuple<Tensor, Tensor, Tensor> convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvShared.cpp:373:std::tuple<at::Tensor,at::Tensor> cudnn_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvPlaceholders.cpp:48:std::tuple<at::Tensor,at::Tensor> cudnn_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:42:static std::tuple<Tensor, Tensor, Tensor> mkldnn_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:915:std::tuple<Tensor, Tensor, Tensor> mkldnn_convolution_backward(

  - convolution_backward(CompositeExplicitAutograd, CUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:59:std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:115:std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_depthwise_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:1420:std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:1441:std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_depthwise_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1977:std::tuple<Tensor, Tensor, Tensor> convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvShared.cpp:373:std::tuple<at::Tensor,at::Tensor> cudnn_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvPlaceholders.cpp:48:std::tuple<at::Tensor,at::Tensor> cudnn_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:42:static std::tuple<Tensor, Tensor, Tensor> mkldnn_convolution_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:915:std::tuple<Tensor, Tensor, Tensor> mkldnn_convolution_backward(

30. cos
  - cos
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:492:Tensor arccos(const Tensor& self) { return self.acos(); }
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:1071:acos(const complex<_Tp>& __x)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:1129:cos(const complex<_Tp>& __x)

31. cosh
  - cosh
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:760:Tensor arccosh(const Tensor& self) { return at::acosh(self); }
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:938:acosh(const complex<_Tp>& __x)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:1020:cosh(const complex<_Tp>& __x)

32. div.Tensor
  - div
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp:460:Tensor quantized_div(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Loss.cpp:239:Tensor kl_div(const Tensor& input, const Tensor& target, int64_t reduction, bool log_target) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:889:Tensor div(const Tensor& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:901:Tensor div(const Tensor& self, const Scalar& other, c10::optional<c10::string_view> rounding_mode) {

  - div_sparse(SparseCPU, SparseCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:257:Tensor div_sparse(const Tensor& self, const Tensor& value) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:274:Tensor div_sparse(const Tensor& self, const Tensor& value, c10::optional<c10::string_view> rounding_mode) {

  - div_zerotensor(ZeroTensor)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1023:Tensor div_zerotensor(const Tensor& self, const Tensor& other) {

  - NestedTensor_div_Tensor(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp:203:Tensor NestedTensor_div_Tensor(const Tensor& self, const Tensor& other) {

33. div.Scalar
  - div
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp:460:Tensor quantized_div(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Loss.cpp:239:Tensor kl_div(const Tensor& input, const Tensor& target, int64_t reduction, bool log_target) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:889:Tensor div(const Tensor& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:901:Tensor div(const Tensor& self, const Scalar& other, c10::optional<c10::string_view> rounding_mode) {

  - div(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp:460:Tensor quantized_div(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Loss.cpp:239:Tensor kl_div(const Tensor& input, const Tensor& target, int64_t reduction, bool log_target) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:889:Tensor div(const Tensor& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:901:Tensor div(const Tensor& self, const Scalar& other, c10::optional<c10::string_view> rounding_mode) {

  - NestedTensor_div_Scalar(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp:211:Tensor NestedTensor_div_Scalar(const Tensor& self, const Scalar& other) {

34. embedding_dense_backward
  - embedding_dense_backward
35. empty.memory_format
  - empty
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesHelper.cpp:37:optional<int64_t> valIfNonempty(optional<int64_t> maybe_empty, int64_t new_val) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:422:c10::optional<int64_t> batch_dim_if_not_empty(const Tensor& t) {
   - /home/haozhe/code/pytorch/aten/src/ATen/NamedTensorUtils.cpp:130:const Tensor& propagate_names_if_present_and_nonempty(const Tensor& result,
   - /home/haozhe/code/pytorch/aten/src/ATen/NamedTensorUtils.cpp:138:const Tensor& propagate_names_if_nonempty(const Tensor& result,
   - /home/haozhe/code/pytorch/aten/src/ATen/NamedTensorUtils.cpp:145:TensorImpl* propagate_names_if_nonempty(TensorImpl* result,

  - empty_cpu(CPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:254:Tensor empty_cpu(IntArrayRef size, c10::optional<ScalarType> dtype_opt, c10::optional<Layout> layout_opt,
   - /home/haozhe/code/pytorch/aten/src/ATen/EmptyTensor.cpp:232:TensorBase empty_cpu(IntArrayRef size, ScalarType dtype, bool pin_memory,
   - /home/haozhe/code/pytorch/aten/src/ATen/EmptyTensor.cpp:239:TensorBase empty_cpu(
   - /home/haozhe/code/pytorch/aten/src/ATen/EmptyTensor.cpp:254:TensorBase empty_cpu(

  - empty_cuda(CUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/EmptyTensor.cpp:9:TensorBase empty_cuda(
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/EmptyTensor.cpp:24:TensorBase empty_cuda(
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/EmptyTensor.cpp:38:TensorBase empty_cuda(

  - empty_mps(MPS)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mps/TensorFactory.cpp:65:Tensor empty_mps(
   - /home/haozhe/code/pytorch/aten/src/ATen/mps/EmptyTensor.cpp:20:TensorBase empty_mps(
   - /home/haozhe/code/pytorch/aten/src/ATen/mps/EmptyTensor.cpp:78:TensorBase empty_mps(

  - empty_meta_symint(Meta)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/MetaTensor.cpp:15:Tensor empty_meta_symint(

  - empty_mkldnn(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorFactories.cpp:15:Tensor empty_mkldnn(IntArrayRef sizes, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorFactories.cpp:29:Tensor empty_mkldnn(IntArrayRef sizes, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<c10::MemoryFormat> optional_memory_format) {

  - empty_sparse(SparseCPU, SparseCUDA, SparseMeta)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp:213:Tensor empty_sparse(

  - empty_sparse_compressed(SparseCsrCPU, SparseCsrCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:558:Tensor empty_sparse_compressed(

  - empty_unknown_quantized(QuantizedCPU, QuantizedCUDA, QuantizedMeta)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/TensorFactories.cpp:71:Tensor empty_unknown_quantized(

36. empty_permuted
  - empty_permuted
37. empty_strided
  - empty_strided
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Factory.cpp:45:Tensor empty_strided(

  - empty_strided_cpu(CPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:321:Tensor empty_strided_cpu(IntArrayRef size, IntArrayRef stride, c10::optional<ScalarType> dtype_opt,
   - /home/haozhe/code/pytorch/aten/src/ATen/EmptyTensor.cpp:265:TensorBase empty_strided_cpu(IntArrayRef size, IntArrayRef stride,
   - /home/haozhe/code/pytorch/aten/src/ATen/EmptyTensor.cpp:273:TensorBase empty_strided_cpu(
   - /home/haozhe/code/pytorch/aten/src/ATen/EmptyTensor.cpp:288:TensorBase empty_strided_cpu(

  - empty_strided_cuda(CUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/EmptyTensor.cpp:49:TensorBase empty_strided_cuda(
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/EmptyTensor.cpp:64:TensorBase empty_strided_cuda(
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/EmptyTensor.cpp:78:TensorBase empty_strided_cuda(

  - empty_strided_mps(MPS)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mps/TensorFactory.cpp:76:Tensor empty_strided_mps(
   - /home/haozhe/code/pytorch/aten/src/ATen/mps/EmptyTensor.cpp:89:TensorBase empty_strided_mps(
   - /home/haozhe/code/pytorch/aten/src/ATen/mps/EmptyTensor.cpp:116:TensorBase empty_strided_mps(

  - empty_strided_meta_symint(Meta)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/MetaTensor.cpp:44:Tensor empty_strided_meta_symint(

  - empty_strided_unknown_quantized(QuantizedCPU, QuantizedCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/TensorFactories.cpp:93:Tensor empty_strided_unknown_quantized(

38. erf
  - erf
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:678:Tensor special_erf(const Tensor& self) { return self.erf(); }

  - erf_sparse(SparseCPU, SparseCUDA)
  - erf_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
39. erfc
  - erfc
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:682:Tensor special_erfc(const Tensor& self) { return self.erfc(); }

40. exp
  - exp
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:2554:Tensor mexp(const Tensor& a, bool compute_highest_degree_approx = false) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:2618:Tensor linalg_matrix_exp(const Tensor& a) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:2636:Tensor matrix_exp(const Tensor& a) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:944:std::tuple<Tensor, Tensor> frexp(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:454:Tensor logcumsumexp(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1410:Tensor logsumexp(const Tensor& self, IntArrayRef dims, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1423:Tensor logsumexp(const Tensor& self, DimnameList dims, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1432:Tensor special_logsumexp(const Tensor& self, IntArrayRef dims, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:2013:Tensor logcumsumexp(const Tensor& self, Dimname dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1560:Tensor ldexp(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:825:exp(const complex<_Tp>& __x)

41. exp2
  - exp2
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:670:Tensor special_exp2(const Tensor& self) { return self.exp2(); }

42. expm1
  - expm1
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:674:Tensor special_expm1(const Tensor& self) { return self.expm1(); }

  - expm1_sparse(SparseCPU, SparseCUDA)
  - expm1_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
43. expand
  - expand
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Expand.cpp:21:Tensor expand(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1103:Tensor expand(const Tensor& self, c10::IntArrayRef size, bool /*unused*/) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:1835:void test_expand(const at::IntArrayRef input_shape, const at::IntArrayRef output_shape) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp:32:bool should_expand(const IntArrayRef &from_size, const IntArrayRef &to_size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/NamedTensorUtils.cpp:382:void propagate_names_for_expand(const Tensor& result, const Tensor& self) {

  - expand(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Expand.cpp:21:Tensor expand(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1103:Tensor expand(const Tensor& self, c10::IntArrayRef size, bool /*unused*/) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:1835:void test_expand(const at::IntArrayRef input_shape, const at::IntArrayRef output_shape) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/scalar_tensor_test.cpp:32:bool should_expand(const IntArrayRef &from_size, const IntArrayRef &to_size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/NamedTensorUtils.cpp:382:void propagate_names_for_expand(const Tensor& result, const Tensor& self) {

44. fill.Scalar
  - fill
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Fill.cpp:75:Tensor fill(const Tensor& self, const Scalar& value) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Fill.cpp:79:Tensor fill(const Tensor& self, const Tensor& value) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:348:Tensor index_fill(const Tensor& self, Dimname dim, const Tensor& index, const Scalar& source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:354:Tensor index_fill(const Tensor& self, Dimname dim, const Tensor& index, const Tensor& source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1478:Tensor index_fill(const Tensor & self, int64_t dim, const Tensor & index, const Scalar& source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1482:Tensor index_fill(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1922:Tensor masked_fill(const Tensor & self, const Tensor & mask, const Scalar& source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1936:Tensor masked_fill(const Tensor & self, const Tensor & mask, const Tensor & source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp:214:Tensor NestedTensor_masked_fill(
   - /home/haozhe/code/pytorch/aten/src/ATen/ScalarOps.cpp:17:Tensor& scalar_fill(Tensor& self, const Scalar& value) {

  - fill(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Fill.cpp:75:Tensor fill(const Tensor& self, const Scalar& value) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Fill.cpp:79:Tensor fill(const Tensor& self, const Tensor& value) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:348:Tensor index_fill(const Tensor& self, Dimname dim, const Tensor& index, const Scalar& source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:354:Tensor index_fill(const Tensor& self, Dimname dim, const Tensor& index, const Tensor& source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1478:Tensor index_fill(const Tensor & self, int64_t dim, const Tensor & index, const Scalar& source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1482:Tensor index_fill(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1922:Tensor masked_fill(const Tensor & self, const Tensor & mask, const Scalar& source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1936:Tensor masked_fill(const Tensor & self, const Tensor & mask, const Tensor & source) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp:214:Tensor NestedTensor_masked_fill(
   - /home/haozhe/code/pytorch/aten/src/ATen/ScalarOps.cpp:17:Tensor& scalar_fill(Tensor& self, const Scalar& value) {

45. floor
  - floor


  - floor_sparse(SparseCPU, SparseCUDA)
  - floor_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
46. full
  - full
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:602:Tensor full(IntArrayRef size, const Scalar& fill_value,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:640:Tensor new_full(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1626:Tensor full(

  - full(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:602:Tensor full(IntArrayRef size, const Scalar& fill_value,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:640:Tensor new_full(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1626:Tensor full(

47. gcd
  - gcd


48. grid_sampler_2d
  - grid_sampler_2d


  - grid_sampler_2d_cpu(CPU, QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/GridSampler.cpp:905:Tensor grid_sampler_2d_cpu(const Tensor& input, const Tensor& grid,

  - grid_sampler_2d_cuda(CUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cuda/GridSampler.cpp:19:Tensor grid_sampler_2d_cuda(const Tensor& input, const Tensor& grid,

  - grid_sampler_2d_mps(MPS)
49. native_group_norm
  - native_group_norm
   - /home/haozhe/code/pytorch/aten/src/ATen/native/group_norm.cpp:64:std::tuple<Tensor, Tensor, Tensor> native_group_norm(

  - native_group_norm(CPU, CUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/group_norm.cpp:64:std::tuple<Tensor, Tensor, Tensor> native_group_norm(

  - math_group_norm(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/group_norm.cpp:214:std::tuple<at::Tensor, at::Tensor, at::Tensor> math_group_norm(

50. native_group_norm_backward
  - native_group_norm_backward
   - /home/haozhe/code/pytorch/aten/src/ATen/native/group_norm.cpp:107:std::tuple<Tensor, Tensor, Tensor> native_group_norm_backward(

  - native_group_norm_backward(CPU, CUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/group_norm.cpp:107:std::tuple<Tensor, Tensor, Tensor> native_group_norm_backward(

51. isnan
  - isnan
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:358:Tensor isnan(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:503:isnan(const complex<_Tp>& __x)

  - isnan(CPU, CUDA, MPS)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:358:Tensor isnan(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:503:isnan(const complex<_Tp>& __x)

  - isnan_sparse(SparseCPU, SparseCUDA)
  - isnan_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
52. native_layer_norm
  - native_layer_norm
   - /home/haozhe/code/pytorch/aten/src/ATen/native/layer_norm.cpp:202:std::tuple<Tensor, Tensor, Tensor> math_native_layer_norm(

  - layer_norm_cpu(CPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/layer_norm.cpp:72:std::tuple<Tensor, Tensor, Tensor> layer_norm_cpu(

  - layer_norm_cuda(CUDA)
  - layer_norm_mps(MPS)
  - math_native_layer_norm(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/layer_norm.cpp:202:std::tuple<Tensor, Tensor, Tensor> math_native_layer_norm(

  - nested_layer_norm(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:146:std::tuple<Tensor, Tensor, Tensor> nested_layer_norm(

53. native_layer_norm_backward
  - native_layer_norm_backward


  - layer_norm_backward_cpu(CPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/layer_norm.cpp:109:std::tuple<Tensor, Tensor, Tensor> layer_norm_backward_cpu(

  - layer_norm_backward_cuda(CUDA)
  - layer_norm_backward_mps(MPS)
  - layer_norm_backward_nested(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorBackward.cpp:194:std::tuple<Tensor, Tensor, Tensor> layer_norm_backward_nested(

54. log
  - log
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:779:log(const complex<_Tp>& __x)

55. log10
  - log10
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:789:log10(const complex<_Tp>& __x)

56. log1p
  - log1p
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:705:Tensor special_log1p(const Tensor& self) { return self.log1p(); }

  - log1p_sparse(SparseCPU, SparseCUDA)
  - log1p_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
57. log2
  - log2
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:799:log2(const complex<_Tp>& __x)

58. _log_softmax
  - _log_softmax
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:569:Tensor special_log_softmax(const Tensor& input, const int64_t dim, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:636:static Tensor _sparse_log_softmax(const Tensor& input_, const int64_t dim_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:645:Tensor _sparse_log_softmax(const Tensor& input_, const int64_t dim_, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:659:Tensor _sparse_log_softmax(const Tensor& self, Dimname dim, optional<ScalarType> dtype) {

59. max.dim
  - max
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp:685:inline void _vec_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp:792:inline void _vec_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp:885:inline void _vec_logsoftmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:154:void host_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:444:static Tensor softmax(const Tensor& input_, const int64_t dim_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:453:Tensor softmax(const Tensor& input_, const int64_t dim_, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:505:Tensor special_softmax(const Tensor& input_, const int64_t dim_, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:509:static Tensor log_softmax(const Tensor& input_, const int64_t dim_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:518:Tensor log_softmax(const Tensor& input_, const int64_t dim_, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:569:Tensor special_log_softmax(const Tensor& input, const int64_t dim, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:583:Tensor softmax(const Tensor& self, Dimname dim, optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:587:Tensor log_softmax(const Tensor& self, Dimname dim, optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:1454:static Tensor _embedding_bag_dense_backward_cpu_max(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Softmax.cpp:140:Tensor softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Softmax.cpp:147:Tensor log_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:158:void cpu_sparse_coo_softmax(Tensor output, const Tensor& input, const int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:609:static Tensor _sparse_softmax(const Tensor& input_, const int64_t dim_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:618:Tensor _sparse_softmax(const Tensor& input_, const int64_t dim_, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:632:Tensor _sparse_softmax(const Tensor& self, Dimname dim, optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:636:static Tensor _sparse_log_softmax(const Tensor& input_, const int64_t dim_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:645:Tensor _sparse_log_softmax(const Tensor& input_, const int64_t dim_, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:659:Tensor _sparse_log_softmax(const Tensor& self, Dimname dim, optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:642:std::tuple<Tensor, Tensor> qmax(const Tensor& self, int64_t dim, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:664:std::tuple<Tensor, Tensor> _aminmax(const Tensor& self, int64_t dim, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:767:std::tuple<Tensor, Tensor> max(const Tensor& self, Dimname dim, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:773:static Tensor argmax(const Tensor& /*self*/, Dimname /*dim*/, bool /*keepdim*/) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/SoftMax.cpp:16:Tensor mkldnn_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/SoftMax.cpp:33:Tensor mkldnn_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qsoftmax.cpp:130:Tensor qsoftmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:815:std::tuple<Tensor, Tensor> cummax(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:2037:std::tuple<Tensor, Tensor> cummax(const Tensor& self, Dimname dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceAllOps.cpp:48:Tensor max(const Tensor &self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/transformers/attention.cpp:141:Tensor masked_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1508:Tensor max(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/EmptyTensor.cpp:19:constexpr uint64_t storage_max() {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:24:const auto int_max = std::numeric_limits<int>::max();
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:26:const auto long_max = std::numeric_limits<int64_t>::max();
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:29:const auto float_max = std::numeric_limits<float>::max();
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:32:const auto double_max = std::numeric_limits<double>::max();

  - qmax(QuantizedCPU, QuantizedCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:642:std::tuple<Tensor, Tensor> qmax(const Tensor& self, int64_t dim, bool keepdim) {

60. amax
  - amax


61. mean.dim
  - mean
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:1513:void _embedding_bag_dense_backward_cpu_sum_mean(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Mean.cpp:13:Tensor mean(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/ReduceOps.cpp:58:static Tensor qnnpack_mean(const Tensor& input, IntArrayRef dim, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1334:Tensor mean(const Tensor &self, optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1338:Tensor mean(const Tensor& self, DimnameList dim, bool keepdim, optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1363:Tensor nanmean(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1806:std::tuple<Tensor, Tensor> var_mean(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1814:std::tuple<Tensor, Tensor> std_mean(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1822:std::tuple<Tensor, Tensor> std_mean(const Tensor& self, bool unbiased) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1828:std::tuple<Tensor, Tensor> var_mean(const Tensor& self, bool unbiased) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1845:std::tuple<Tensor, Tensor> var_mean(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1854:std::tuple<Tensor, Tensor> std_mean(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1940:std::tuple<Tensor,Tensor> var_mean(const Tensor& self, DimnameList dim, bool unbiased, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1944:std::tuple<Tensor,Tensor> std_mean(const Tensor& self, DimnameList dim, bool unbiased, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1967:std::tuple<Tensor,Tensor> var_mean(const Tensor& self, DimnameList dim,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1972:std::tuple<Tensor,Tensor> std_mean(const Tensor& self, DimnameList dim,

  - mean_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/ReduceOps.cpp:165:Tensor mean_quantized_cpu(

62. min.dim
  - min
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:653:std::tuple<Tensor, Tensor> qmin(const Tensor& self, int64_t dim, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:761:std::tuple<Tensor, Tensor> min(const Tensor& self, Dimname dim, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:776:static Tensor argmin(const Tensor& /*self*/, Dimname /*dim*/, bool /*keepdim*/) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:212:static void check_argmax_argmin(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:850:std::tuple<Tensor, Tensor> cummin(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:2043:std::tuple<Tensor, Tensor> cummin(const Tensor& self, Dimname dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceAllOps.cpp:26:Tensor min(const Tensor &self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1517:Tensor min(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:23:const auto int_min = std::numeric_limits<int>::min();
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:25:const auto long_min = std::numeric_limits<int64_t>::min();
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:28:const auto float_min = std::numeric_limits<float>::min();
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:31:const auto double_min = std::numeric_limits<double>::min();

  - qmin(QuantizedCPU, QuantizedCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:653:std::tuple<Tensor, Tensor> qmin(const Tensor& self, int64_t dim, bool keepdim) {

63. amin
  - amin


64. mm
  - mm
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Mm.cpp:340:Tensor addmm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Mm.cpp:354:Tensor mm(const Tensor& mat1_arg, const Tensor& mat2_arg) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:264:void common_checks_baddbmm_bmm(Meta& meta, const Tensor& batch1, const Tensor& batch2, const Scalar& beta, const Scalar& alpha, bool is_bmm, const c10::optional<Tensor>& self_baddbmm = nullopt) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1518:Tensor addbmm(const Tensor& self, const Tensor& batch1, const Tensor& batch2, const Scalar& beta, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1362:Tensor _sparse_addmm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1375:Tensor _sparse_mm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1400:Tensor _sparse_mm(const Tensor& mat1, const Tensor& mat2, const c10::string_view reduce) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1613:Tensor smm(const Tensor& self, const Tensor& mat2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1620:Tensor sspaddmm(const Tensor& self, const Tensor& mat1, const Tensor& mat2,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:742:Tensor _sparse_csr_mm(const Tensor& mat1, const Tensor& mat2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp:466:void block_sparse_mm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp:576:void spmm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp:686:void spgemm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorTransformerFunctions.cpp:90:Tensor NestedTensor_times_Tensor_plus_Tensor_addmm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/CPUBlas.cpp:78:bool use_blas_gemm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/CPUBlas.cpp:93:fbgemm::matrix_op_t to_fbgemm(TransposeType trans) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/CPUBlas.cpp:118:void gemm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/CPUBlas.cpp:161:void gemm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/CPUBlas.cpp:204:void gemm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/CPUBlas.cpp:247:void gemm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/CPUBlas.cpp:290:void gemm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/CPUBlas.cpp:330:void gemm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:28:bool mkldnn_bf16_gemm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:56:bool mkldnn_bf16_gemm(
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CUDABlas.cpp:892:void int8_gemm(
   - /home/haozhe/code/pytorch/aten/src/ATen/NamedTensorUtils.cpp:356:std::vector<Dimname> propagate_names_for_addmm(

  - _sparse_mm(SparseCPU, SparseCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1375:Tensor _sparse_mm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1400:Tensor _sparse_mm(const Tensor& mat1, const Tensor& mat2, const c10::string_view reduce) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp:466:void block_sparse_mm(

  - _sparse_csr_mm(SparseCsrCPU, SparseCsrCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:742:Tensor _sparse_csr_mm(const Tensor& mat1, const Tensor& mat2) {

65. mul.Tensor
  - mul
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp:451:Tensor quantized_mul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1079:Tensor chain_matmul(TensorList matrices) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:2068:Tensor matmul(const Tensor & tensor1, const Tensor & tensor2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:2084:Tensor linalg_matmul(const Tensor & tensor1, const Tensor & tensor2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorTransformerFunctions.cpp:76:Tensor NestedTensor_matmul(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:12:void mkldnn_matmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:21:bool use_mkldnn_bf16_matmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:50:static bool use_mkldnn_bf16_matmul() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:121:void mkldnn_matmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:238:bool use_mkldnn_bf16_matmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:40:Tensor mkldnn_mul(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:144:Tensor mkldnn_mul(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qmatmul.cpp:32:Tensor qmatmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qmatmul.cpp:166:Tensor qmatmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:993:Tensor mul(const Tensor& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1546:Tensor _test_serialization_subcmul(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/native_test.cpp:129:void TestMatmul(TensorOptions T, Tensor& t, TensorOptions AccT) {

  - mul_sparse(SparseCPU, SparseCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:811:Tensor mul_sparse(const Tensor& self, const Tensor& other) {

  - mul_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:279:Tensor mul_sparse_csr(const Tensor& self, const Tensor& other) {

  - mkldnn_mul(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:40:Tensor mkldnn_mul(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:144:Tensor mkldnn_mul(const Tensor& self, const Tensor& other) {

  - mul_zerotensor(ZeroTensor)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1014:Tensor mul_zerotensor(const Tensor& self, const Tensor& other) {

  - NestedTensor_mul_Tensor(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp:191:Tensor NestedTensor_mul_Tensor(const Tensor& self, const Tensor& other) {

66. mul.Scalar
  - mul
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp:451:Tensor quantized_mul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1079:Tensor chain_matmul(TensorList matrices) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:2068:Tensor matmul(const Tensor & tensor1, const Tensor & tensor2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:2084:Tensor linalg_matmul(const Tensor & tensor1, const Tensor & tensor2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorTransformerFunctions.cpp:76:Tensor NestedTensor_matmul(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:12:void mkldnn_matmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:21:bool use_mkldnn_bf16_matmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:50:static bool use_mkldnn_bf16_matmul() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:121:void mkldnn_matmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:238:bool use_mkldnn_bf16_matmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:40:Tensor mkldnn_mul(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:144:Tensor mkldnn_mul(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qmatmul.cpp:32:Tensor qmatmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qmatmul.cpp:166:Tensor qmatmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:993:Tensor mul(const Tensor& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1546:Tensor _test_serialization_subcmul(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/native_test.cpp:129:void TestMatmul(TensorOptions T, Tensor& t, TensorOptions AccT) {

  - mul(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp:451:Tensor quantized_mul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1079:Tensor chain_matmul(TensorList matrices) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:2068:Tensor matmul(const Tensor & tensor1, const Tensor & tensor2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:2084:Tensor linalg_matmul(const Tensor & tensor1, const Tensor & tensor2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorTransformerFunctions.cpp:76:Tensor NestedTensor_matmul(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:12:void mkldnn_matmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:21:bool use_mkldnn_bf16_matmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:50:static bool use_mkldnn_bf16_matmul() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:121:void mkldnn_matmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Matmul.cpp:238:bool use_mkldnn_bf16_matmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:40:Tensor mkldnn_mul(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/BinaryOps.cpp:144:Tensor mkldnn_mul(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qmatmul.cpp:32:Tensor qmatmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qmatmul.cpp:166:Tensor qmatmul(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:993:Tensor mul(const Tensor& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1546:Tensor _test_serialization_subcmul(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/native_test.cpp:129:void TestMatmul(TensorOptions T, Tensor& t, TensorOptions AccT) {

  - mul_scalar_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:387:Tensor mul_scalar_sparse_csr(const Tensor& self, const Scalar& other) {

  - NestedTensor_mul_Scalar(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp:199:Tensor NestedTensor_mul_Scalar(const Tensor& self, const Scalar& other) {

67. native_batch_norm
  - native_batch_norm


  - batch_norm_cpu(CPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Normalization.cpp:748:std::tuple<Tensor, Tensor, Tensor> batch_norm_cpu(const Tensor& self, const c10::optional<Tensor>& weight_opt, const c10::optional<Tensor>& bias_opt, const c10::optional<Tensor>& running_mean_opt, const c10::optional<Tensor>& running_var_opt,

  - batch_norm_cuda(CUDA)
  - batch_norm_mps(MPS)
  - mkldnn_batch_norm(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Normalization.cpp:21:std::tuple<Tensor, Tensor, Tensor> mkldnn_batch_norm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Normalization.cpp:117:std::tuple<Tensor, Tensor, Tensor> mkldnn_batch_norm(

68. _native_batch_norm_legit.no_stats
  - _native_batch_norm_legit


  - _batch_norm_legit_no_stats_cpu(CPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Normalization.cpp:804:std::tuple<Tensor, Tensor, Tensor> _batch_norm_legit_no_stats_cpu(

  - _batch_norm_legit_no_stats_cuda(CUDA)
  - _batch_norm_legit_no_stats_mps(MPS)
  - _mkldnn_batch_norm_legit_no_stats(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Normalization.cpp:54:std::tuple<Tensor, Tensor, Tensor> _mkldnn_batch_norm_legit_no_stats(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Normalization.cpp:204:std::tuple<Tensor, Tensor, Tensor> _mkldnn_batch_norm_legit_no_stats(

69. permute
  - permute
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/RNN_miopen.cpp:281:void _copyParams_and_permute(MatrixRef<Tensor> params_from, MatrixRef<Tensor> params_to, int64_t mode) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Permute.cpp:75:Tensor permute(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1413:Tensor permute(const Tensor& self, IntArrayRef dims) {

  - permute(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/RNN_miopen.cpp:281:void _copyParams_and_permute(MatrixRef<Tensor> params_from, MatrixRef<Tensor> params_to, int64_t mode) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Permute.cpp:75:Tensor permute(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1413:Tensor permute(const Tensor& self, IntArrayRef dims) {

  - permute_mps(MPS)
  - permute_sparse_coo(SparseCPU, SparseCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1420:Tensor permute_sparse_coo(const Tensor& self, IntArrayRef dims) {

70. scalar_tensor
  - scalar_tensor
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:16:static bool is_allowed_dim_on_scalar_tensor(int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:351:static bool is_allowed_dim_on_scalar_tensor(int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:71:static bool is_allowed_dim_on_scalar_tensor(int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:755:Tensor scalar_tensor(const Scalar& s,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:444:Tensor qadd_scalar_tensor(Tensor qa, Tensor b) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:61:static bool is_allowed_dim_on_scalar_tensor(int64_t dim) {

  - scalar_tensor(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:16:static bool is_allowed_dim_on_scalar_tensor(int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:351:static bool is_allowed_dim_on_scalar_tensor(int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:71:static bool is_allowed_dim_on_scalar_tensor(int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:755:Tensor scalar_tensor(const Scalar& s,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp:444:Tensor qadd_scalar_tensor(Tensor qa, Tensor b) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:61:static bool is_allowed_dim_on_scalar_tensor(int64_t dim) {

71. rand
  - rand
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:781:Tensor rand(IntArrayRef size,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:789:Tensor rand(IntArrayRef size, c10::optional<Generator> generator,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1693:Tensor rand(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1703:Tensor rand(
   - /home/haozhe/code/pytorch/aten/src/ATen/nnapi/nnapi_wrapper.cpp:144:static int check_Model_addOperand(ANeuralNetworksModel* model, const ANeuralNetworksOperandType* type) {
   - /home/haozhe/code/pytorch/aten/src/ATen/TensorIterator.cpp:832:void TensorIteratorBase::remove_operand(int arg) {
   - /home/haozhe/code/pytorch/aten/src/ATen/TensorIterator.cpp:836:void TensorIteratorBase::unsafe_replace_operand(int arg, void* data) {

  - rand(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:781:Tensor rand(IntArrayRef size,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:789:Tensor rand(IntArrayRef size, c10::optional<Generator> generator,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1693:Tensor rand(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1703:Tensor rand(
   - /home/haozhe/code/pytorch/aten/src/ATen/nnapi/nnapi_wrapper.cpp:144:static int check_Model_addOperand(ANeuralNetworksModel* model, const ANeuralNetworksOperandType* type) {
   - /home/haozhe/code/pytorch/aten/src/ATen/TensorIterator.cpp:832:void TensorIteratorBase::remove_operand(int arg) {
   - /home/haozhe/code/pytorch/aten/src/ATen/TensorIterator.cpp:836:void TensorIteratorBase::unsafe_replace_operand(int arg, void* data) {

72. randn
  - randn
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:1168:In [2]: t=torch.randn(5000, 5000).to_sparse_csr()
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:930:Tensor randn(IntArrayRef size,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:938:Tensor randn(IntArrayRef size, c10::optional<Generator> generator,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1668:Tensor randn(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1678:Tensor randn(

  - randn(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:1168:In [2]: t=torch.randn(5000, 5000).to_sparse_csr()
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:930:Tensor randn(IntArrayRef size,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:938:Tensor randn(IntArrayRef size, c10::optional<Generator> generator,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1668:Tensor randn(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1678:Tensor randn(

73. reciprocal
  - reciprocal


74. neg
  - neg
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:641:Tensor resolve_neg(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:857:Tensor negative(const Tensor& self) { return self.neg(); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:105:Tensor NestedTensor_neg(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TypeProperties.cpp:59:bool is_neg(const Tensor& self) {

  - neg_sparse(SparseCPU, SparseCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:151:SparseTensor neg_sparse(const SparseTensor& t) {

  - neg_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
  - NestedTensor_neg(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:105:Tensor NestedTensor_neg(const Tensor& self) {

75. repeat
  - repeat
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Repeat.cpp:21:Tensor repeat(const Tensor& self, const IntArrayRef repeats) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1478:Tensor repeat(const Tensor& self, IntArrayRef repeats) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:3156:void test_repeat(

  - repeat(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Repeat.cpp:21:Tensor repeat(const Tensor& self, const IntArrayRef repeats) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1478:Tensor repeat(const Tensor& self, IntArrayRef repeats) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:3156:void test_repeat(

  - repeat_mps(MPS)
76. reshape
  - reshape
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1632:Tensor reshape(const Tensor& self, IntArrayRef proposed_shape) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:26:Tensor mkldnn_reshape(const Tensor& self, IntArrayRef size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:57:Tensor mkldnn_reshape(const Tensor& self, IntArrayRef size) {

  - reshape_symint(CompositeImplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1569:Tensor reshape_symint(const Tensor& self, c10::SymIntArrayRef proposed_shape) {

  - reshape_nested(CompositeImplicitAutogradNestedTensor)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:861:Tensor reshape_nested(const Tensor& self, IntArrayRef proposed_shape) {

77. round
  - round
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/QueryPool.cpp:20:constexpr int64_t default_ns_per_tick = 52; // lround(52.08f);
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:709:Tensor special_round(const Tensor& self, int64_t decimals) { return self.round(decimals); }

  - round_sparse(SparseCPU, SparseCUDA)
  - round_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:476:Tensor round_sparse_csr(const Tensor& self) {

78. relu
  - relu
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:123:at::Tensor miopen_convolution_add_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:130:at::Tensor miopen_convolution_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:1548:Tensor miopen_convolution_add_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:1604:Tensor miopen_convolution_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Activation.cpp:507:Tensor relu(const Tensor & self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Activation.cpp:666:Tensor rrelu(const Tensor & self, const Scalar& lower, const Scalar& upper, bool training, c10::optional<Generator> generator) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Activation.cpp:684:Tensor prelu(const Tensor& self, const Tensor& weight_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:244:Tensor relu(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:432:Tensor leaky_relu(const Tensor& self_arg, const Scalar& negative_slope) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear.cpp:222:at::Tensor PackedLinearWeight::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_dynamic.cpp:156:at::Tensor PackedLinearWeightQnnp::apply_dynamic_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:65:Tensor NestedTensor_relu(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvShared.cpp:431:Tensor cudnn_convolution_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvShared.cpp:501:Tensor cudnn_convolution_add_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvPlaceholders.cpp:106:Tensor cudnn_convolution_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvPlaceholders.cpp:117:Tensor cudnn_convolution_add_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Prelu.cpp:10:Tensor mkldnn_prelu(const Tensor& input, const Tensor& weight) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Prelu.cpp:27:Tensor mkldnn_prelu(const Tensor& input, const Tensor& weight) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Relu.cpp:16:Tensor mkldnn_relu(const Tensor& input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Relu.cpp:37:Tensor mkldnn_relu(const Tensor& input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:243:at::Tensor PackedConvWeight<kSpatialDim>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:534:template at::Tensor PackedConvWeight<2>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:544:template at::Tensor PackedConvWeight<3>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1054:at::Tensor PackedConvWeightsQnnp<kSpatialDim>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1072:template at::Tensor PackedConvWeightsQnnp<2>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1082:template at::Tensor PackedConvWeightsQnnp<3>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1109:at::Tensor PackedConvWeightsOnednn<kSpatialDim>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1127:at::Tensor PackedConvWeightsOnednn<kSpatialDim>::apply_add_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1362:template at::Tensor PackedConvWeightsOnednn<2>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1372:template at::Tensor PackedConvWeightsOnednn<3>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:249:at::Tensor PackedLinearWeight::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:764:at::Tensor PackedLinearWeightsQnnp::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:877:at::Tensor PackedLinearWeightsOnednn::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:885:at::Tensor PackedLinearWeightsOnednn:: apply_leaky_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp:233:at::Tensor PackedLinearWeight::apply_dynamic_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp:395:at::Tensor PackedLinearWeightsQnnp::apply_dynamic_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp:458:at::Tensor PackedLinearWeightFp16::apply_dynamic_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp:599:at::Tensor PackedLinearWeightsOnednn::apply_dynamic_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qrelu.cpp:39:static Tensor qnnpack_relu(Tensor input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cudnn/Linear.cpp:332:at::Tensor PackedLinearWeightCudnn::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cudnn/Conv.cpp:326:at::Tensor PackedConvWeightCudnn<kSpatialDim>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cudnn/Conv.cpp:338:template at::Tensor PackedConvWeightCudnn<2>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:849:Tensor add_relu(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:854:Tensor add_relu(const Tensor& self, const Scalar& other, const Scalar& alpha) {

  - relu(CPU, CUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:123:at::Tensor miopen_convolution_add_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:130:at::Tensor miopen_convolution_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:1548:Tensor miopen_convolution_add_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp:1604:Tensor miopen_convolution_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Activation.cpp:507:Tensor relu(const Tensor & self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Activation.cpp:666:Tensor rrelu(const Tensor & self, const Scalar& lower, const Scalar& upper, bool training, c10::optional<Generator> generator) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Activation.cpp:684:Tensor prelu(const Tensor& self, const Tensor& weight_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:244:Tensor relu(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:432:Tensor leaky_relu(const Tensor& self_arg, const Scalar& negative_slope) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear.cpp:222:at::Tensor PackedLinearWeight::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_dynamic.cpp:156:at::Tensor PackedLinearWeightQnnp::apply_dynamic_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:65:Tensor NestedTensor_relu(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvShared.cpp:431:Tensor cudnn_convolution_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvShared.cpp:501:Tensor cudnn_convolution_add_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvPlaceholders.cpp:106:Tensor cudnn_convolution_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/ConvPlaceholders.cpp:117:Tensor cudnn_convolution_add_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Prelu.cpp:10:Tensor mkldnn_prelu(const Tensor& input, const Tensor& weight) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Prelu.cpp:27:Tensor mkldnn_prelu(const Tensor& input, const Tensor& weight) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Relu.cpp:16:Tensor mkldnn_relu(const Tensor& input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Relu.cpp:37:Tensor mkldnn_relu(const Tensor& input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:243:at::Tensor PackedConvWeight<kSpatialDim>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:534:template at::Tensor PackedConvWeight<2>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:544:template at::Tensor PackedConvWeight<3>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1054:at::Tensor PackedConvWeightsQnnp<kSpatialDim>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1072:template at::Tensor PackedConvWeightsQnnp<2>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1082:template at::Tensor PackedConvWeightsQnnp<3>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1109:at::Tensor PackedConvWeightsOnednn<kSpatialDim>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1127:at::Tensor PackedConvWeightsOnednn<kSpatialDim>::apply_add_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1362:template at::Tensor PackedConvWeightsOnednn<2>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1372:template at::Tensor PackedConvWeightsOnednn<3>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:249:at::Tensor PackedLinearWeight::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:764:at::Tensor PackedLinearWeightsQnnp::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:877:at::Tensor PackedLinearWeightsOnednn::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:885:at::Tensor PackedLinearWeightsOnednn:: apply_leaky_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp:233:at::Tensor PackedLinearWeight::apply_dynamic_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp:395:at::Tensor PackedLinearWeightsQnnp::apply_dynamic_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp:458:at::Tensor PackedLinearWeightFp16::apply_dynamic_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp:599:at::Tensor PackedLinearWeightsOnednn::apply_dynamic_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qrelu.cpp:39:static Tensor qnnpack_relu(Tensor input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cudnn/Linear.cpp:332:at::Tensor PackedLinearWeightCudnn::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cudnn/Conv.cpp:326:at::Tensor PackedConvWeightCudnn<kSpatialDim>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cudnn/Conv.cpp:338:template at::Tensor PackedConvWeightCudnn<2>::apply_relu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:849:Tensor add_relu(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:854:Tensor add_relu(const Tensor& self, const Scalar& other, const Scalar& alpha) {

  - relu_mps(MPS)
  - mkldnn_relu(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Relu.cpp:16:Tensor mkldnn_relu(const Tensor& input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Relu.cpp:37:Tensor mkldnn_relu(const Tensor& input) {

  - relu_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qrelu.cpp:106:Tensor relu_quantized_cpu(const Tensor& qx) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qrelu.cpp:138:Tensor leaky_relu_quantized_cpu(const Tensor& self, const Scalar& negval) {

  - relu_quantized_cuda(QuantizedCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cuda/Activation.cpp:21:Tensor relu_quantized_cuda(const Tensor& self) {

  - NestedTensor_relu(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:65:Tensor NestedTensor_relu(const Tensor& self) {

  - relu_sparse(SparseCPU, SparseCUDA)
  - relu_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
79. gelu
  - gelu
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:400:Tensor gelu(const Tensor& self_arg, c10::string_view approximate) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:77:Tensor NestedTensor_gelu(const Tensor& self, c10::string_view approximate) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Gelu.cpp:17:Tensor mkldnn_gelu(const Tensor& input, c10::string_view approximate) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Gelu.cpp:34:Tensor mkldnn_gelu(const Tensor& input, c10::string_view approximate) {

  - mkldnn_gelu(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Gelu.cpp:17:Tensor mkldnn_gelu(const Tensor& input, c10::string_view approximate) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Gelu.cpp:34:Tensor mkldnn_gelu(const Tensor& input, c10::string_view approximate) {

  - gelu_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qgelu.cpp:16:Tensor gelu_quantized_cpu(const Tensor& qx, c10::string_view approximate) {

  - gelu_quantized_cuda(QuantizedCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cuda/Activation.cpp:11:Tensor gelu_quantized_cuda(const Tensor& qx, c10::string_view approximate) {

  - NestedTensor_gelu(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:77:Tensor NestedTensor_gelu(const Tensor& self, c10::string_view approximate) {

80. rsqrt
  - rsqrt


81. select.int
  - select
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:89:bool is_fast_path_index_select(const Tensor& src, Tensor& output, index_t padding_idx) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Select.cpp:389:Tensor select(const Tensor& self, int64_t dim, int64_t index) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:369:Tensor index_select(const Tensor& self, Dimname dim, const Tensor& index) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1758:Tensor select(const Tensor& self, int64_t dim, int64_t index) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1762:Tensor select(const Tensor& self, Dimname dim, int64_t index) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:3250:void test_select(const at::IntArrayRef input_shape, int64_t dim, int64_t index) {

  - select_symint(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1766:Tensor select_symint(const Tensor& self, int64_t dim, c10::SymInt index) {

  - select_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:1113:Tensor select_sparse_csr(const Tensor& self, int64_t dim, int64_t index) {

  - select_nested(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:422:Tensor select_nested(const Tensor& self, int64_t dim, int64_t index) {

82. sigmoid
  - sigmoid
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Activation.cpp:777:Tensor log_sigmoid(const Tensor & self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:260:Tensor hardsigmoid(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:444:Tensor sigmoid(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/UnaryOps.cpp:17:Tensor mkldnn_sigmoid(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/UnaryOps.cpp:43:Tensor mkldnn_sigmoid(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qhardsigmoid.cpp:27:Tensor qnnpack_hardsigmoid(Tensor input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qsigmoid.cpp:29:static Tensor qnnpack_sigmoid(

  - sigmoid_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qhardsigmoid.cpp:90:Tensor hardsigmoid_quantized_cpu(const Tensor& qx) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qsigmoid.cpp:100:Tensor sigmoid_quantized_cpu(const Tensor& qx) {

  - mkldnn_sigmoid(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/UnaryOps.cpp:17:Tensor mkldnn_sigmoid(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/UnaryOps.cpp:43:Tensor mkldnn_sigmoid(const Tensor& self) {

83. sin
  - sin
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:533:Tensor arcsin(const Tensor& self) { return self.asin(); }
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:1061:asin(const complex<_Tp>& __x)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:1118:sin(const complex<_Tp>& __x)

  - sin_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
  - sin_sparse(SparseCPU, SparseCUDA)
84. sinh
  - sinh
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:765:Tensor arcsinh(const Tensor& self) { return self.asinh(); }
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:909:asinh(const complex<_Tp>& __x)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:1005:sinh(const complex<_Tp>& __x)

  - sinh_sparse(SparseCPU, SparseCUDA)
  - sinh_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
85. slice.Tensor
  - slice
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp:33:void _fft_fill_with_conjugate_symmetry_slice(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Slice.cpp:232:Tensor slice(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:2448:Tensor slice(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/RNN.cpp:644:Tensor hidden_slice(const Tensor& t, int64_t start, int64_t end) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/RNN.cpp:647:tpair_of<Tensor> hidden_slice(const tpair_of<Tensor>& t, int64_t start, int64_t end) {

  - slice(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkl/SpectralOps.cpp:33:void _fft_fill_with_conjugate_symmetry_slice(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Slice.cpp:232:Tensor slice(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:2448:Tensor slice(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/RNN.cpp:644:Tensor hidden_slice(const Tensor& t, int64_t start, int64_t end) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/RNN.cpp:647:tpair_of<Tensor> hidden_slice(const tpair_of<Tensor>& t, int64_t start, int64_t end) {

86. slice_scatter
  - slice_scatter
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3898:at::Tensor slice_scatter(const at::Tensor& self, const at::Tensor& src, int64_t dim, c10::optional<int64_t> start, c10::optional<int64_t> end, int64_t step) {

  - slice_scatter(CompositeExplicitAutogradNonFunctional)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3898:at::Tensor slice_scatter(const at::Tensor& self, const at::Tensor& src, int64_t dim, c10::optional<int64_t> start, c10::optional<int64_t> end, int64_t step) {

87. as_strided_scatter
  - as_strided_scatter


  - as_strided_scatter_symint(CompositeExplicitAutogradNonFunctional)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3921:at::Tensor as_strided_scatter_symint(const at::Tensor& self, const at::Tensor& src, at::SymIntArrayRef size, at::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset) {

88. _softmax
  - _softmax
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp:685:inline void _vec_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SoftMaxKernel.cpp:792:inline void _vec_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:154:void host_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:505:Tensor special_softmax(const Tensor& input_, const int64_t dim_, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:509:static Tensor log_softmax(const Tensor& input_, const int64_t dim_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:518:Tensor log_softmax(const Tensor& input_, const int64_t dim_, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:569:Tensor special_log_softmax(const Tensor& input, const int64_t dim, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SoftMax.cpp:587:Tensor log_softmax(const Tensor& self, Dimname dim, optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Softmax.cpp:147:Tensor log_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:158:void cpu_sparse_coo_softmax(Tensor output, const Tensor& input, const int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:609:static Tensor _sparse_softmax(const Tensor& input_, const int64_t dim_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:618:Tensor _sparse_softmax(const Tensor& input_, const int64_t dim_, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:632:Tensor _sparse_softmax(const Tensor& self, Dimname dim, optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:636:static Tensor _sparse_log_softmax(const Tensor& input_, const int64_t dim_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:645:Tensor _sparse_log_softmax(const Tensor& input_, const int64_t dim_, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SoftMax.cpp:659:Tensor _sparse_log_softmax(const Tensor& self, Dimname dim, optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/SoftMax.cpp:16:Tensor mkldnn_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/SoftMax.cpp:33:Tensor mkldnn_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/transformers/attention.cpp:141:Tensor masked_softmax(

  - mkldnn_softmax(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/SoftMax.cpp:16:Tensor mkldnn_softmax(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/SoftMax.cpp:33:Tensor mkldnn_softmax(

  - softmax_nested(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:499:Tensor softmax_nested(

89. squeeze
  - squeeze
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyVmapTransforms.cpp:160:static Tensor moveDimToFrontAndUnsqueeze(Tensor tensor, optional<int64_t> dim, int64_t example_ndim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Unsqueeze.cpp:17:Tensor unsqueeze(const at::Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:405:Tensor squeeze(const Tensor& self, Dimname dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3180:Tensor squeeze(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3192:Tensor squeeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3208:Tensor squeeze(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3276:Tensor unsqueeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:3957:void test_unsqueeze(const at::IntArrayRef input_shape, int64_t dim) {

  - squeeze(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyVmapTransforms.cpp:160:static Tensor moveDimToFrontAndUnsqueeze(Tensor tensor, optional<int64_t> dim, int64_t example_ndim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Unsqueeze.cpp:17:Tensor unsqueeze(const at::Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:405:Tensor squeeze(const Tensor& self, Dimname dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3180:Tensor squeeze(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3192:Tensor squeeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3208:Tensor squeeze(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3276:Tensor unsqueeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:3957:void test_unsqueeze(const at::IntArrayRef input_shape, int64_t dim) {

  - squeeze_quantized(QuantizedCPU, QuantizedCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3188:Tensor squeeze_quantized(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3204:Tensor squeeze_quantized(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3217:Tensor squeeze_quantized(const Tensor& self, IntArrayRef dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3307:Tensor unsqueeze_quantized(const Tensor& self, int64_t dim) {

  - squeeze_nested(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:566:Tensor squeeze_nested(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:621:Tensor unsqueeze_nested(const Tensor& self, int64_t dim) {

90. squeeze.dim
  - squeeze
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyVmapTransforms.cpp:160:static Tensor moveDimToFrontAndUnsqueeze(Tensor tensor, optional<int64_t> dim, int64_t example_ndim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Unsqueeze.cpp:17:Tensor unsqueeze(const at::Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:405:Tensor squeeze(const Tensor& self, Dimname dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3180:Tensor squeeze(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3192:Tensor squeeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3208:Tensor squeeze(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3276:Tensor unsqueeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:3957:void test_unsqueeze(const at::IntArrayRef input_shape, int64_t dim) {

  - squeeze(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyVmapTransforms.cpp:160:static Tensor moveDimToFrontAndUnsqueeze(Tensor tensor, optional<int64_t> dim, int64_t example_ndim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Unsqueeze.cpp:17:Tensor unsqueeze(const at::Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:405:Tensor squeeze(const Tensor& self, Dimname dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3180:Tensor squeeze(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3192:Tensor squeeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3208:Tensor squeeze(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3276:Tensor unsqueeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:3957:void test_unsqueeze(const at::IntArrayRef input_shape, int64_t dim) {

  - squeeze_quantized(QuantizedCPU, QuantizedCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3188:Tensor squeeze_quantized(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3204:Tensor squeeze_quantized(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3217:Tensor squeeze_quantized(const Tensor& self, IntArrayRef dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3307:Tensor unsqueeze_quantized(const Tensor& self, int64_t dim) {

  - squeeze_dim_nested(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:574:Tensor squeeze_dim_nested(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:617:Tensor squeeze_dim_nested(const Tensor& self, int64_t dim) {

91. squeeze.dims
  - squeeze
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyVmapTransforms.cpp:160:static Tensor moveDimToFrontAndUnsqueeze(Tensor tensor, optional<int64_t> dim, int64_t example_ndim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Unsqueeze.cpp:17:Tensor unsqueeze(const at::Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:405:Tensor squeeze(const Tensor& self, Dimname dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3180:Tensor squeeze(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3192:Tensor squeeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3208:Tensor squeeze(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3276:Tensor unsqueeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:3957:void test_unsqueeze(const at::IntArrayRef input_shape, int64_t dim) {

  - squeeze(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyVmapTransforms.cpp:160:static Tensor moveDimToFrontAndUnsqueeze(Tensor tensor, optional<int64_t> dim, int64_t example_ndim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Unsqueeze.cpp:17:Tensor unsqueeze(const at::Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:405:Tensor squeeze(const Tensor& self, Dimname dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3180:Tensor squeeze(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3192:Tensor squeeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3208:Tensor squeeze(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3276:Tensor unsqueeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:3957:void test_unsqueeze(const at::IntArrayRef input_shape, int64_t dim) {

  - squeeze_quantized(QuantizedCPU, QuantizedCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3188:Tensor squeeze_quantized(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3204:Tensor squeeze_quantized(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3217:Tensor squeeze_quantized(const Tensor& self, IntArrayRef dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3307:Tensor unsqueeze_quantized(const Tensor& self, int64_t dim) {

  - squeeze_dim_nested(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:574:Tensor squeeze_dim_nested(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:617:Tensor squeeze_dim_nested(const Tensor& self, int64_t dim) {

92. sum
  - sum
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/BlasKernel.cpp:34:auto sum(int64_t N, Func f) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:332:std::array<scalar_t, nrows> multi_row_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:404:scalar_t row_sum(const char * C10_RESTRICT in_data,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:425:void vectorized_inner_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:455:void scalar_inner_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:467:void vectorized_outer_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:506:void scalar_outer_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:529:void cascade_sum(TensorIterator &iter) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/cumsum.cpp:13:Tensor cumsum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Linear.cpp:237:Tensor einsum(c10::string_view equation, TensorList operands, at::OptionalIntArrayRef path) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1634:Tensor _sparse_sum(const SparseTensor& input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1638:Tensor _sparse_sum(const SparseTensor& input, ScalarType dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1644:Tensor _sparse_sum(const SparseTensor& input, IntArrayRef dims_to_sum, ScalarType dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1648:Tensor _sparse_sum(const SparseTensor& input, IntArrayRef dims_to_sum) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:1170:In [3]: %timeit torch._sparse_csr_sum(t, dim=(0, 1), keepdim=True)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:1173:In [4]: %timeit torch.sum(t.values())
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp:223:int64_t hsum(const uint8_t* A, int len) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp:280:int64_t hsum(const int8_t* A, int len) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp:337:int64_t hsum(const int32_t* A, int len) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:506:static Tensor reversed_cumsum(const Tensor& w, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1173:Tensor sum(const Tensor &self, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1177:Tensor sum(const Tensor& self, DimnameList dim, bool keepdim, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1208:Tensor nansum(const Tensor& self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<ScalarType> opt_dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:2019:Tensor cumsum(const Tensor& self, Dimname dim, c10::optional<ScalarType> dtype) {

  - sum(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/BlasKernel.cpp:34:auto sum(int64_t N, Func f) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:332:std::array<scalar_t, nrows> multi_row_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:404:scalar_t row_sum(const char * C10_RESTRICT in_data,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:425:void vectorized_inner_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:455:void scalar_inner_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:467:void vectorized_outer_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:506:void scalar_outer_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:529:void cascade_sum(TensorIterator &iter) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/cumsum.cpp:13:Tensor cumsum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Linear.cpp:237:Tensor einsum(c10::string_view equation, TensorList operands, at::OptionalIntArrayRef path) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1634:Tensor _sparse_sum(const SparseTensor& input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1638:Tensor _sparse_sum(const SparseTensor& input, ScalarType dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1644:Tensor _sparse_sum(const SparseTensor& input, IntArrayRef dims_to_sum, ScalarType dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1648:Tensor _sparse_sum(const SparseTensor& input, IntArrayRef dims_to_sum) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:1170:In [3]: %timeit torch._sparse_csr_sum(t, dim=(0, 1), keepdim=True)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:1173:In [4]: %timeit torch.sum(t.values())
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp:223:int64_t hsum(const uint8_t* A, int len) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp:280:int64_t hsum(const int8_t* A, int len) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp:337:int64_t hsum(const int32_t* A, int len) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:506:static Tensor reversed_cumsum(const Tensor& w, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1173:Tensor sum(const Tensor &self, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1177:Tensor sum(const Tensor& self, DimnameList dim, bool keepdim, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1208:Tensor nansum(const Tensor& self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<ScalarType> opt_dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:2019:Tensor cumsum(const Tensor& self, Dimname dim, c10::optional<ScalarType> dtype) {

  - sum_coo(SparseCPU, SparseCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:2140:Tensor sum_coo(const Tensor &self, c10::optional<ScalarType> dtype) {

  - sum_csr(SparseCsrCPU, SparseCsrCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:2136:Tensor sum_csr(const Tensor &self, c10::optional<ScalarType> dtype) {

93. sum.dim_IntList
  - sum
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/BlasKernel.cpp:34:auto sum(int64_t N, Func f) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:332:std::array<scalar_t, nrows> multi_row_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:404:scalar_t row_sum(const char * C10_RESTRICT in_data,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:425:void vectorized_inner_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:455:void scalar_inner_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:467:void vectorized_outer_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:506:void scalar_outer_sum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/SumKernel.cpp:529:void cascade_sum(TensorIterator &iter) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/cumsum.cpp:13:Tensor cumsum(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Linear.cpp:237:Tensor einsum(c10::string_view equation, TensorList operands, at::OptionalIntArrayRef path) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1634:Tensor _sparse_sum(const SparseTensor& input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1638:Tensor _sparse_sum(const SparseTensor& input, ScalarType dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1644:Tensor _sparse_sum(const SparseTensor& input, IntArrayRef dims_to_sum, ScalarType dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1648:Tensor _sparse_sum(const SparseTensor& input, IntArrayRef dims_to_sum) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:1170:In [3]: %timeit torch._sparse_csr_sum(t, dim=(0, 1), keepdim=True)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:1173:In [4]: %timeit torch.sum(t.values())
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp:223:int64_t hsum(const uint8_t* A, int len) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp:280:int64_t hsum(const int8_t* A, int len) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp:337:int64_t hsum(const int32_t* A, int len) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:506:static Tensor reversed_cumsum(const Tensor& w, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1173:Tensor sum(const Tensor &self, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1177:Tensor sum(const Tensor& self, DimnameList dim, bool keepdim, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1208:Tensor nansum(const Tensor& self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<ScalarType> opt_dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:2019:Tensor cumsum(const Tensor& self, Dimname dim, c10::optional<ScalarType> dtype) {

  - NestedTensor_sum_dim_CPU(NestedTensorCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:351:Tensor NestedTensor_sum_dim_CPU(

  - sum_sparse_coo(SparseCPU, SparseCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:2144:Tensor sum_sparse_coo(const Tensor& self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<ScalarType> dtype) {

94. sqrt
  - sqrt
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:808:sqrt(const complex<_Tp>& __x)

  - sqrt_sparse(SparseCPU, SparseCUDA)
  - sqrt_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
95. prod
  - prod
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Itertools.cpp:48:Tensor cartesian_prod(TensorList tensors) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1258:static void impl_func_prod(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1281:Tensor prod(const Tensor &self, c10::optional<ScalarType> opt_dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1289:Tensor prod(const Tensor& self, Dimname dim, bool keepdim, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:2028:Tensor cumprod(const Tensor& self, Dimname dim, c10::optional<ScalarType> dtype) {

  - prod(CPU, CUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Itertools.cpp:48:Tensor cartesian_prod(TensorList tensors) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1258:static void impl_func_prod(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1281:Tensor prod(const Tensor &self, c10::optional<ScalarType> opt_dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1289:Tensor prod(const Tensor& self, Dimname dim, bool keepdim, c10::optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:2028:Tensor cumprod(const Tensor& self, Dimname dim, c10::optional<ScalarType> dtype) {

  - prod_mps(MPS)
96. tan
  - tan
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:538:Tensor arctan(const Tensor& self) { return self.atan(); }
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:1108:atan(const complex<_Tp>& __x)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:1138:tan(const complex<_Tp>& __x)

  - tan_sparse(SparseCPU, SparseCUDA)
  - tan_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
97. tanh
  - tanh
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Activation.cpp:429:Tensor hardtanh(const Tensor& self, const Scalar& min, const Scalar& max) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:236:Tensor hardtanh(const Tensor& self, const Scalar& min, const Scalar& max) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:452:Tensor tanh(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:770:Tensor arctanh(const Tensor& self) { return self.atanh(); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:93:Tensor NestedTensor_tanh(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/UnaryOps.cpp:25:Tensor mkldnn_tanh(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/UnaryOps.cpp:59:Tensor mkldnn_tanh(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:896:at::Tensor PackedLinearWeightsOnednn:: apply_tanh(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qtanh.cpp:25:static Tensor qnnpack_tanh(Tensor input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:972:atanh(const complex<_Tp>& __x)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:1037:tanh(const complex<_Tp>& __x)

  - tanh_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qclamp.cpp:141:Tensor hardtanh_quantized_cpu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qtanh.cpp:92:Tensor tanh_quantized_cpu(const Tensor& qx) {

  - mkldnn_tanh(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/UnaryOps.cpp:25:Tensor mkldnn_tanh(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/UnaryOps.cpp:59:Tensor mkldnn_tanh(const Tensor& self) {

  - tanh_sparse(SparseCPU, SparseCUDA)
  - tanh_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
  - NestedTensor_tanh(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp:93:Tensor NestedTensor_tanh(const Tensor& self) {

98. flip
  - flip
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorTransformations.cpp:37:Tensor flip(const Tensor& self, IntArrayRef dims) {

  - flip(CPU, QuantizedCPU, CUDA, QuantizedCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorTransformations.cpp:37:Tensor flip(const Tensor& self, IntArrayRef dims) {

  - flip_mps(MPS)
99. trunc
  - trunc
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:848:Tensor fix(const Tensor& self) { return self.trunc(); }

  - trunc_sparse(SparseCPU, SparseCUDA)
  - trunc_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
100. unsqueeze
  - unsqueeze
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Unsqueeze.cpp:17:Tensor unsqueeze(const at::Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3276:Tensor unsqueeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:3957:void test_unsqueeze(const at::IntArrayRef input_shape, int64_t dim) {

  - unsqueeze(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Unsqueeze.cpp:17:Tensor unsqueeze(const at::Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3276:Tensor unsqueeze(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:3957:void test_unsqueeze(const at::IntArrayRef input_shape, int64_t dim) {

  - unsqueeze_sparse(SparseCPU, SparseCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3282:Tensor unsqueeze_sparse(Tensor const &self, int64_t dim) {

  - unsqueeze_quantized(QuantizedCPU, QuantizedCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3307:Tensor unsqueeze_quantized(const Tensor& self, int64_t dim) {

  - unsqueeze_nested(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:621:Tensor unsqueeze_nested(const Tensor& self, int64_t dim) {

101. var
  - var
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1863:Tensor var(const Tensor& self, bool unbiased) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1869:Tensor var(const Tensor& self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1916:Tensor var(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1931:Tensor var(const Tensor& self, DimnameList dim, bool unbiased, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1957:Tensor var(const Tensor& self, DimnameList dim, const c10::optional<Scalar>& correction, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/ParallelCommon.cpp:22:const char* get_env_var(

102. var.dim
  - var
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1863:Tensor var(const Tensor& self, bool unbiased) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1869:Tensor var(const Tensor& self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1916:Tensor var(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1931:Tensor var(const Tensor& self, DimnameList dim, bool unbiased, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1957:Tensor var(const Tensor& self, DimnameList dim, const c10::optional<Scalar>& correction, bool keepdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/ParallelCommon.cpp:22:const char* get_env_var(

103. where.self
  - where
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:2434:Tensor argwhere(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:525:Tensor where(const Tensor& condition, const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:533:Tensor where(const Tensor& condition, const Scalar& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:540:Tensor where(const Tensor& condition, const Tensor& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:547:Tensor where(const Tensor& condition, const Scalar& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:554:std::vector<Tensor> where(const Tensor& condition) {

  - where(CPU, CUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:2434:Tensor argwhere(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:525:Tensor where(const Tensor& condition, const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:533:Tensor where(const Tensor& condition, const Scalar& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:540:Tensor where(const Tensor& condition, const Tensor& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:547:Tensor where(const Tensor& condition, const Scalar& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:554:std::vector<Tensor> where(const Tensor& condition) {

  - where_mps(MPS)
104. where
  - where
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:2434:Tensor argwhere(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:525:Tensor where(const Tensor& condition, const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:533:Tensor where(const Tensor& condition, const Scalar& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:540:Tensor where(const Tensor& condition, const Tensor& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:547:Tensor where(const Tensor& condition, const Scalar& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:554:std::vector<Tensor> where(const Tensor& condition) {

105. clone
  - clone
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clone.cpp:17:Tensor clone(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1598:Tensor clone(const Tensor& src, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:30:Tensor mkldnn_clone(const Tensor& self, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:69:Tensor mkldnn_clone(const Tensor& self, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/QTensor.cpp:189:Tensor quantized_clone(
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:337:std::shared_ptr<CPUGeneratorImpl> CPUGeneratorImpl::clone() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CUDAGeneratorImpl.cpp:342:std::shared_ptr<CUDAGeneratorImpl> CUDAGeneratorImpl::clone() const {

  - clone(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clone.cpp:17:Tensor clone(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1598:Tensor clone(const Tensor& src, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:30:Tensor mkldnn_clone(const Tensor& self, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:69:Tensor mkldnn_clone(const Tensor& self, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/QTensor.cpp:189:Tensor quantized_clone(
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:337:std::shared_ptr<CPUGeneratorImpl> CPUGeneratorImpl::clone() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CUDAGeneratorImpl.cpp:342:std::shared_ptr<CUDAGeneratorImpl> CUDAGeneratorImpl::clone() const {

  - clone_sparse(SparseCPU, SparseCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp:467:SparseTensor clone_sparse(

  - clone_sparse_compressed(SparseCsrCPU, SparseCsrCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:734:SparseCsrTensor clone_sparse_compressed(

  - mkldnn_clone(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:30:Tensor mkldnn_clone(const Tensor& self, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:69:Tensor mkldnn_clone(const Tensor& self, c10::optional<c10::MemoryFormat> optional_memory_format) {

  - quantized_clone(QuantizedCPU, QuantizedCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/QTensor.cpp:189:Tensor quantized_clone(

  - clone_nested(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorFactories.cpp:133:Tensor clone_nested(

106. sub.Tensor
  - sub
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp:442:Tensor quantized_sub(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1052:static Tensor maybe_add_maybe_sub(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1125:Tensor sub(const Tensor& self, const Scalar& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1154:Tensor rsub(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1188:Tensor rsub(const Tensor& self, const Scalar& other, const Scalar& alpha) {

  - sub_sparse(SparseCPU, SparseCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:432:Tensor sub_sparse(const Tensor& self, const Tensor& other, const Scalar& alpha) {

  - sub_zerotensor(ZeroTensor)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1080:Tensor sub_zerotensor(const Tensor& self, const Tensor& other, const Scalar& alpha) {

  - NestedTensor_sub_Tensor(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp:181:Tensor NestedTensor_sub_Tensor(

107. sub.Scalar
  - sub
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp:442:Tensor quantized_sub(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1052:static Tensor maybe_add_maybe_sub(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1125:Tensor sub(const Tensor& self, const Scalar& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1154:Tensor rsub(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1188:Tensor rsub(const Tensor& self, const Scalar& other, const Scalar& alpha) {

  - sub(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp:442:Tensor quantized_sub(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1052:static Tensor maybe_add_maybe_sub(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1125:Tensor sub(const Tensor& self, const Scalar& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1154:Tensor rsub(const Tensor& self, const Tensor& other, const Scalar& alpha) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1188:Tensor rsub(const Tensor& self, const Scalar& other, const Scalar& alpha) {

108. addmm
  - addmm
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Mm.cpp:340:Tensor addmm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1362:Tensor _sparse_addmm(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1620:Tensor sspaddmm(const Tensor& self, const Tensor& mat1, const Tensor& mat2,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorTransformerFunctions.cpp:90:Tensor NestedTensor_times_Tensor_plus_Tensor_addmm(
   - /home/haozhe/code/pytorch/aten/src/ATen/NamedTensorUtils.cpp:356:std::vector<Dimname> propagate_names_for_addmm(

  - addmm_sparse_dense_cpu(SparseCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1327:static Tensor s_addmm_sparse_dense_cpu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:1339:Tensor addmm_sparse_dense_cpu(

  - addmm_sparse_dense_cuda(SparseCUDA)
  - addmm_sparse_compressed_dense(SparseCsrCPU, SparseCsrCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:723:Tensor addmm_sparse_compressed_dense(

109. _to_copy
  - _to_copy
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorConversions.cpp:225:Tensor _to_copy(

  - _to_copy(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorConversions.cpp:225:Tensor _to_copy(

  - _to_copy_nested(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorFactories.cpp:84:Tensor _to_copy_nested(

110. item
  - item
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Scalar.cpp:17:Scalar item(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/TensorIndexing.cpp:45:static inline void set_item(const Tensor& self, ArrayRef<TensorIndex> indices, const Scalar& v) {

111. view
  - view
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Shape.cpp:45:inline Tensor view(const Tensor& self_arg, IntArrayRef shape) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TestOps.cpp:106:Tensor _test_autograd_multiple_dispatch_view(const Tensor &self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:595:Tensor _neg_view(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3272:Tensor _unsafe_view(const Tensor& self, IntArrayRef size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3648:Tensor view(const Tensor& self,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:22:Tensor mkldnn_view(const Tensor& self, IntArrayRef size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:52:Tensor mkldnn_view(const Tensor& self, IntArrayRef size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/Tensor.cpp:151:bool TensorBase::is_view() const {

  - view(ZeroTensor, Meta, CPU, CUDA, QuantizedCPU, QuantizedCUDA, MPS)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Shape.cpp:45:inline Tensor view(const Tensor& self_arg, IntArrayRef shape) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TestOps.cpp:106:Tensor _test_autograd_multiple_dispatch_view(const Tensor &self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:595:Tensor _neg_view(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3272:Tensor _unsafe_view(const Tensor& self, IntArrayRef size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3648:Tensor view(const Tensor& self,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:22:Tensor mkldnn_view(const Tensor& self, IntArrayRef size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:52:Tensor mkldnn_view(const Tensor& self, IntArrayRef size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/Tensor.cpp:151:bool TensorBase::is_view() const {

  - mkldnn_view(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:22:Tensor mkldnn_view(const Tensor& self, IntArrayRef size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:52:Tensor mkldnn_view(const Tensor& self, IntArrayRef size) {

  - view_nested(NestedTensorCPU, NestedTensorCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:774:Tensor view_nested(const Tensor& self, IntArrayRef proposed_shape) {

112. scatter_add
  - scatter_add
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/GridSamplerKernel.cpp:442:mask_scatter_add(const scalar_t *src, scalar_t* base_addr,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:384:Tensor scatter_add(const Tensor& self, Dimname dim, const Tensor& index, const Tensor& source) {

113. scatter_reduce.two
  - scatter_reduce
114. bitwise_and.Tensor
  - bitwise_and
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1196:Tensor bitwise_and(const Tensor& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1200:Tensor bitwise_and(const Scalar& self, const Tensor& other) {

115. bitwise_or.Tensor
  - bitwise_or
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1229:Tensor bitwise_or(const Tensor& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1233:Tensor bitwise_or(const Scalar& self, const Tensor& other) {

116. bitwise_xor.Tensor
  - bitwise_xor
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1262:Tensor bitwise_xor(const Tensor& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1266:Tensor bitwise_xor(const Scalar& self, const Tensor& other) {

117. ne.Scalar
  - ne
   - /home/haozhe/code/pytorch/aten/src/ATen/Context.cpp:286:at::QEngine Context::qEngine() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/Context.cpp:312:void Context::setQEngine(at::QEngine e) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp:18:static at::Tensor linspace_from_neg_one(const Tensor& grid, int64_t num_steps,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Pipeline.cpp:150:ComputePipeline::ComputePipeline(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Pipeline.cpp:212:ComputePipeline::ComputePipeline(ComputePipeline&& other) noexcept
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Pipeline.cpp:217:ComputePipeline::~ComputePipeline() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Command.cpp:77:void CommandBuffer::bind_pipeline(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clone.cpp:17:Tensor clone(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1598:Tensor clone(const Tensor& src, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/RowwisePrune.cpp:93:std::tuple<Tensor, Tensor> _rowwise_prune(const Tensor& weights,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:30:Tensor mkldnn_clone(const Tensor& self, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:69:Tensor mkldnn_clone(const Tensor& self, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/QTensor.cpp:189:Tensor quantized_clone(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/AffineQuantizer.cpp:117:Tensor& quantize_tensor_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/AffineQuantizer.cpp:147:Tensor& quantize_tensor_per_channel_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/AffineQuantizer.cpp:214:Tensor& dequantize_tensor_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/AffineQuantizer.cpp:240:Tensor& dequantize_tensor_per_channel_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/FakeQuantPerTensorAffine.cpp:31:Tensor fake_quantize_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/FakeQuantPerTensorAffine.cpp:42:Tensor fake_quantize_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/FakeQuantPerTensorAffine.cpp:148:Tensor _fake_quantize_learnable_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/FakeQuantPerChannelAffine.cpp:32:Tensor fake_quantize_per_channel_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/FakeQuantPerChannelAffine.cpp:139:Tensor _fake_quantize_learnable_per_channel_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1476:Tensor not_equal(const Tensor& self, const Tensor& other) { return self.ne(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1479:Tensor not_equal(const Tensor& self, const Scalar& other) { return self.ne(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:319:at::mt19937 CPUGeneratorImpl::engine() {
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:328:void CPUGeneratorImpl::set_engine(at::mt19937 engine) {
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:337:std::shared_ptr<CPUGeneratorImpl> CPUGeneratorImpl::clone() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/class_type.cpp:406:ClassTypePtr ClassType::refine(at::ArrayRef<TypePtr> refined_slots) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:245:void test_pow_one(const std::vector<T> vals) {
   - /home/haozhe/code/pytorch/aten/src/ATen/TensorIterator.cpp:1695:bool DimCounter::is_done() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CUDAGeneratorImpl.cpp:342:std::shared_ptr<CUDAGeneratorImpl> CUDAGeneratorImpl::clone() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/quantized/Quantizer.cpp:321:Tensor from_blob_quantized_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/quantized/Quantizer.cpp:358:Tensor from_blob_quantized_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/quantized/Quantizer.cpp:387:Tensor from_blob_quantized_per_channel_affine(

  - ne_quantized_cpu(QuantizedCPU)
118. ne.Tensor
  - ne
   - /home/haozhe/code/pytorch/aten/src/ATen/Context.cpp:286:at::QEngine Context::qEngine() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/Context.cpp:312:void Context::setQEngine(at::QEngine e) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/AffineGridGenerator.cpp:18:static at::Tensor linspace_from_neg_one(const Tensor& grid, int64_t num_steps,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Pipeline.cpp:150:ComputePipeline::ComputePipeline(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Pipeline.cpp:212:ComputePipeline::ComputePipeline(ComputePipeline&& other) noexcept
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Pipeline.cpp:217:ComputePipeline::~ComputePipeline() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Command.cpp:77:void CommandBuffer::bind_pipeline(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clone.cpp:17:Tensor clone(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1598:Tensor clone(const Tensor& src, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/RowwisePrune.cpp:93:std::tuple<Tensor, Tensor> _rowwise_prune(const Tensor& weights,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:30:Tensor mkldnn_clone(const Tensor& self, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/TensorShape.cpp:69:Tensor mkldnn_clone(const Tensor& self, c10::optional<c10::MemoryFormat> optional_memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/QTensor.cpp:189:Tensor quantized_clone(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/AffineQuantizer.cpp:117:Tensor& quantize_tensor_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/AffineQuantizer.cpp:147:Tensor& quantize_tensor_per_channel_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/AffineQuantizer.cpp:214:Tensor& dequantize_tensor_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/AffineQuantizer.cpp:240:Tensor& dequantize_tensor_per_channel_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/FakeQuantPerTensorAffine.cpp:31:Tensor fake_quantize_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/FakeQuantPerTensorAffine.cpp:42:Tensor fake_quantize_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/FakeQuantPerTensorAffine.cpp:148:Tensor _fake_quantize_learnable_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/FakeQuantPerChannelAffine.cpp:32:Tensor fake_quantize_per_channel_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/FakeQuantPerChannelAffine.cpp:139:Tensor _fake_quantize_learnable_per_channel_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1476:Tensor not_equal(const Tensor& self, const Tensor& other) { return self.ne(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1479:Tensor not_equal(const Tensor& self, const Scalar& other) { return self.ne(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:319:at::mt19937 CPUGeneratorImpl::engine() {
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:328:void CPUGeneratorImpl::set_engine(at::mt19937 engine) {
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:337:std::shared_ptr<CPUGeneratorImpl> CPUGeneratorImpl::clone() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/class_type.cpp:406:ClassTypePtr ClassType::refine(at::ArrayRef<TypePtr> refined_slots) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:245:void test_pow_one(const std::vector<T> vals) {
   - /home/haozhe/code/pytorch/aten/src/ATen/TensorIterator.cpp:1695:bool DimCounter::is_done() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CUDAGeneratorImpl.cpp:342:std::shared_ptr<CUDAGeneratorImpl> CUDAGeneratorImpl::clone() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/quantized/Quantizer.cpp:321:Tensor from_blob_quantized_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/quantized/Quantizer.cpp:358:Tensor from_blob_quantized_per_tensor_affine(
   - /home/haozhe/code/pytorch/aten/src/ATen/quantized/Quantizer.cpp:387:Tensor from_blob_quantized_per_channel_affine(

  - ne_quantized_cpu(QuantizedCPU)
119. eq.Scalar
  - eq
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SpectralOps.cpp:719:Tensor fft_fftfreq(int64_t n, double d,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SpectralOps.cpp:740:Tensor fft_rfftfreq(int64_t n, double d,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mps/operations/Equal.cpp:15:TORCH_API at::Tensor eq(const at::Tensor & self, const at::Tensor & other);
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:91:void assert_eq(T val, T act, T exp) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:100:void assert_eq(T val, T act, T exp) {

  - eq_quantized_cpu(QuantizedCPU)
120. eq.Tensor
  - eq
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SpectralOps.cpp:719:Tensor fft_fftfreq(int64_t n, double d,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/SpectralOps.cpp:740:Tensor fft_rfftfreq(int64_t n, double d,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mps/operations/Equal.cpp:15:TORCH_API at::Tensor eq(const at::Tensor & self, const at::Tensor & other);
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:91:void assert_eq(T val, T act, T exp) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:100:void assert_eq(T val, T act, T exp) {

  - eq_quantized_cpu(QuantizedCPU)
121. ge.Scalar
  - ge
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchedTensorImpl.cpp:96:bool BatchedTensorImpl::has_storage() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Resize.cpp:182:static void _maybe_resize_storage(TensorImpl* self, int64_t new_size_bytes) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Resize.cpp:186:static void _maybe_resize_storage(TensorImpl* self, c10::SymInt new_size_bytes) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Shader.cpp:189:void ShaderLayoutCache::purge() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Shader.cpp:222:void ShaderCache::purge() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Pipeline.cpp:48:VkPipelineStageFlags vk_stage(const PipelineStageFlags stage) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Pipeline.cpp:274:void PipelineLayoutCache::purge() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Pipeline.cpp:337:void ComputePipelineCache::purge() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Resource.cpp:314:VulkanImage::VulkanImage()
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Resource.cpp:328:VulkanImage::VulkanImage(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Resource.cpp:419:VulkanImage::VulkanImage(VulkanImage&& other) noexcept
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Resource.cpp:455:VulkanImage::~VulkanImage() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Resource.cpp:525:void SamplerCache::purge() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Resource.cpp:581:VulkanImage MemoryAllocator::create_image(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Tensor.cpp:268:api::VulkanImage& vTensor::image(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Tensor.cpp:277:api::VulkanImage& vTensor::image(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Tensor.cpp:319:api::VulkanImage allocate_image(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Tensor.cpp:381:vTensorStorage::vTensorStorage(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Tensor.cpp:398:vTensorStorage::~vTensorStorage() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TestOps.cpp:97:Tensor _test_autograd_multiple_dispatch_fullcoverage(const Tensor &self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1145:static void check_indexarray_range(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:65:bool solve_arange(const Tensor& input, int64_t& start, int64_t& end, int64_t& step) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:134:Tensor arange(const Scalar& end,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:142:Tensor arange(const Scalar& start, const Scalar& end,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:151:Tensor arange(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:186:Tensor _dim_arange(const Tensor& like, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1065:Tensor range(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1080:Tensor range(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/fused_obs_fake_quant.cpp:28:void calculate_moving_average(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1450:Tensor greater_equal(const Tensor& self, const Tensor& other) { return self.ge(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1453:Tensor greater_equal(const Tensor& self, const Scalar& other) { return self.ge(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/SparseTensorImpl.cpp:70:bool SparseTensorImpl::has_storage() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchedTensorImpl.cpp:106:bool BatchedTensorImpl::has_storage() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/SavedTensorHooks.cpp:38:const c10::optional<std::string>& SavedTensorDefaultHooks::get_disabled_error_message() {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/class_type.cpp:73:std::string ClassType::getForwardPreHookErrorMessage(int pre_hook_idx) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/class_type.cpp:99:std::string ClassType::getForwardHookErrorMessage(int hook_idx) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/type.cpp:1063:SymbolicShape SymbolicShape::merge(const SymbolicShape& other) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/tensor_type.cpp:55:VaryingShape<T> VaryingShape<T>::merge(const VaryingShape<T>& other) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/tensor_type.cpp:349:TensorTypePtr TensorType::merge(const TensorType& other, bool merge_sizes) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/StorageUtils.cpp:8:C10_EXPORT c10::intrusive_ptr<c10::StorageImpl> new_shm_fd_storage(
   - /home/haozhe/code/pytorch/aten/src/ATen/FunctionalTensorWrapper.cpp:64:void FunctionalTensorWrapper::freeze_storage() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/FunctionalTensorWrapper.cpp:233:void FunctionalTensorWrapper::maybe_replace_storage(const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/Exceptions.cpp:51:C10_EXPORT const char* cusolverGetErrorMessage(cusolverStatus_t status) {
   - /home/haozhe/code/pytorch/aten/src/ATen/TensorUtils.cpp:48:void checkDimRange(CheckedFrom c, const TensorGeometryArg& t, int64_t dim_start, int64_t dim_end) {

  - ge_quantized_cpu(QuantizedCPU)
122. ge.Tensor
  - ge
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchedTensorImpl.cpp:96:bool BatchedTensorImpl::has_storage() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Resize.cpp:182:static void _maybe_resize_storage(TensorImpl* self, int64_t new_size_bytes) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Resize.cpp:186:static void _maybe_resize_storage(TensorImpl* self, c10::SymInt new_size_bytes) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Shader.cpp:189:void ShaderLayoutCache::purge() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Shader.cpp:222:void ShaderCache::purge() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Pipeline.cpp:48:VkPipelineStageFlags vk_stage(const PipelineStageFlags stage) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Pipeline.cpp:274:void PipelineLayoutCache::purge() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Pipeline.cpp:337:void ComputePipelineCache::purge() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Resource.cpp:314:VulkanImage::VulkanImage()
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Resource.cpp:328:VulkanImage::VulkanImage(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Resource.cpp:419:VulkanImage::VulkanImage(VulkanImage&& other) noexcept
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Resource.cpp:455:VulkanImage::~VulkanImage() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Resource.cpp:525:void SamplerCache::purge() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Resource.cpp:581:VulkanImage MemoryAllocator::create_image(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Tensor.cpp:268:api::VulkanImage& vTensor::image(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Tensor.cpp:277:api::VulkanImage& vTensor::image(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Tensor.cpp:319:api::VulkanImage allocate_image(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Tensor.cpp:381:vTensorStorage::vTensorStorage(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Tensor.cpp:398:vTensorStorage::~vTensorStorage() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TestOps.cpp:97:Tensor _test_autograd_multiple_dispatch_fullcoverage(const Tensor &self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1145:static void check_indexarray_range(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:65:bool solve_arange(const Tensor& input, int64_t& start, int64_t& end, int64_t& step) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:134:Tensor arange(const Scalar& end,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:142:Tensor arange(const Scalar& start, const Scalar& end,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:151:Tensor arange(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:186:Tensor _dim_arange(const Tensor& like, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1065:Tensor range(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1080:Tensor range(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/fused_obs_fake_quant.cpp:28:void calculate_moving_average(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1450:Tensor greater_equal(const Tensor& self, const Tensor& other) { return self.ge(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1453:Tensor greater_equal(const Tensor& self, const Scalar& other) { return self.ge(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/SparseTensorImpl.cpp:70:bool SparseTensorImpl::has_storage() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchedTensorImpl.cpp:106:bool BatchedTensorImpl::has_storage() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/SavedTensorHooks.cpp:38:const c10::optional<std::string>& SavedTensorDefaultHooks::get_disabled_error_message() {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/class_type.cpp:73:std::string ClassType::getForwardPreHookErrorMessage(int pre_hook_idx) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/class_type.cpp:99:std::string ClassType::getForwardHookErrorMessage(int hook_idx) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/type.cpp:1063:SymbolicShape SymbolicShape::merge(const SymbolicShape& other) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/tensor_type.cpp:55:VaryingShape<T> VaryingShape<T>::merge(const VaryingShape<T>& other) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/tensor_type.cpp:349:TensorTypePtr TensorType::merge(const TensorType& other, bool merge_sizes) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/StorageUtils.cpp:8:C10_EXPORT c10::intrusive_ptr<c10::StorageImpl> new_shm_fd_storage(
   - /home/haozhe/code/pytorch/aten/src/ATen/FunctionalTensorWrapper.cpp:64:void FunctionalTensorWrapper::freeze_storage() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/FunctionalTensorWrapper.cpp:233:void FunctionalTensorWrapper::maybe_replace_storage(const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/Exceptions.cpp:51:C10_EXPORT const char* cusolverGetErrorMessage(cusolverStatus_t status) {
   - /home/haozhe/code/pytorch/aten/src/ATen/TensorUtils.cpp:48:void checkDimRange(CheckedFrom c, const TensorGeometryArg& t, int64_t dim_start, int64_t dim_end) {

  - ge_quantized_cpu(QuantizedCPU)
123. le.Scalar
  - le
   - /home/haozhe/code/pytorch/aten/src/ATen/Context.cpp:356:bool Context::isXNNPACKAvailable() {
   - /home/haozhe/code/pytorch/aten/src/ATen/miopen/Handle.cpp:9:void createMIOpenHandle(miopenHandle_t *handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/miopen/Handle.cpp:13:void destroyMIOpenHandle(miopenHandle_t handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/miopen/Handle.cpp:35:miopenHandle_t getMiopenHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesNorm.cpp:46:batch_norm_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesNorm.cpp:127:std::tuple<at::Tensor,optional<int64_t>> batch_norm_backward_no_weight_bias_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesNorm.cpp:352:static std::tuple<at::Tensor,optional<int64_t>> group_norm_backward_no_weight_bias_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesNorm.cpp:508:native_layer_norm_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesNorm.cpp:553:static std::tuple<at::Tensor,optional<int64_t>> native_layer_norm_backward_no_weight_bias_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:25:static std::tuple<Tensor, optional<int64_t>> _is_all_true_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:30:static std::tuple<Tensor, optional<int64_t>> _is_any_true_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:106:void boxed_reduction_batch_rule(const c10::OperatorHandle& op, torch::jit::Stack* stack) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:261:static std::tuple<Tensor,optional<int64_t>> _softmax_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:291:static std::tuple<Tensor,optional<int64_t>> _log_softmax_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:319:static std::tuple<Tensor,optional<int64_t>> searchsorted_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:94:std::tuple<Tensor,optional<int64_t>> unsqueeze_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:105:std::tuple<Tensor,optional<int64_t>> repeat_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:120:std::tuple<Tensor,optional<int64_t>> _unsafe_view_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:141:std::tuple<Tensor,optional<int64_t>> flip_batch_rule(const Tensor& self, optional<int64_t> self_bdim, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:191:std::tuple<Tensor, optional<int64_t>> squeeze_batch_rule(const Tensor& self, optional<int64_t> bdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:226:std::tuple<Tensor, optional<int64_t>> squeeze_dims_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:258:std::tuple<Tensor, optional<int64_t>> squeeze_dim_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:263:std::tuple<Tensor, optional<int64_t>> select_batching_rule(const Tensor& self, optional<int64_t> bdim, int64_t dim, c10::SymInt index) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:274:std::tuple<Tensor, optional<int64_t>> _reshape_alias_batch_rule(const Tensor& self, optional<int64_t> bdim, const c10::SymIntArrayRef shape, const c10::SymIntArrayRef strides) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:285:std::tuple<Tensor, optional<int64_t>> roll_batch_rule(const Tensor& self, optional<int64_t> bdim, SymIntArrayRef shifts, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:311:std::tuple<Tensor, optional<int64_t>> diagonal_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:323:std::tuple<Tensor,optional<int64_t>> diagonal_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:337:std::tuple<Tensor,optional<int64_t>> slice_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:356:transpose_int_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:377:std::tuple<Tensor, optional<int64_t>> permute_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:395:std::tuple<Tensor,optional<int64_t>> select_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:408:std::tuple<Tensor,optional<int64_t>> slice_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:421:std::tuple<Tensor, optional<int64_t>> view_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:433:std::tuple<Tensor,optional<int64_t>> view_copy_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:447:std::tuple<Tensor, optional<int64_t>> expand_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:481:std::tuple<Tensor, optional<int64_t>> unfold_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:498:std::tuple<Tensor, optional<int64_t>> narrow_copy_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:510:std::tuple<std::vector<Tensor>, optional<int64_t>> unsafe_split_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:523:std::tuple<Tensor, optional<int64_t>> movedim_batch_rule(const Tensor& self, optional<int64_t> self_bdim, IntArrayRef source, IntArrayRef destination) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:530:std::tuple<Tensor, optional<int64_t>> diag_embed_batch_rule(const Tensor& self, optional<int64_t> self_bdim, int64_t offset, int64_t dim1, int64_t dim2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:543:std::tuple<Tensor,optional<int64_t>> tril_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:553:std::tuple<Tensor,optional<int64_t>> triu_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesUnaryOps.cpp:14:clone_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesUnaryOps.cpp:52:view_as_complex_batch_rule(const Tensor& self, optional<int64_t> self_bdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesUnaryOps.cpp:63:to_other_batch_rule(const Tensor& self, optional<int64_t> self_bdim,
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:25:std::tuple<Tensor, optional<int64_t>> dot_batch_rule(const Tensor& A, optional<int64_t> A_bdim, const Tensor& B, optional<int64_t> B_bdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:42:static std::tuple<Tensor, optional<int64_t>> tv_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:69:static std::tuple<Tensor, optional<int64_t>> mv_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:82:static std::tuple<Tensor, optional<int64_t>> mm_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:97:static std::tuple<Tensor, optional<int64_t>> bmm_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:169:void _linalg_check_errors_batch_rule(const Tensor& info, optional<int64_t> info_bdim, c10::string_view api_name, bool is_matrix) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:176:householder_product_batch_rule(const Tensor &input, c10::optional<int64_t> input_bdim,
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:283:oneOutput linalg_lu_solve_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:325:oneOutput cholesky_solve_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:340:threeOutputs linalg_lu_factor_ex_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:348:twoOutputs linalg_lu_factor_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:356:oneOutput matrix_exp_batch_rule(const Tensor& self, c10::optional<int64_t> self_bdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:362:fourOutputs solve_ex_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:404:oneOutput cross_batch_rule(const Tensor& self, c10::optional<int64_t> self_bdim,
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:429:fourOutputs linalg_lstsq_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:455:atol_rtol_tensor_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:484:pinv_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesModules.cpp:23:static std::tuple<Tensor,optional<int64_t>> embedding_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesModules.cpp:54:embedding_dense_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesModules.cpp:113:grid_sample_batch_rule(const Tensor& input, optional<int64_t> input_bdim, const Tensor& grid, optional<int64_t> grid_bdim, ExtraArgs... extra_args) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesModules.cpp:179:grid_sample_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesModules.cpp:200:cudnn_grid_sample_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLoss.cpp:53:mse_loss_batch_rule(const at::Tensor& self, optional<int64_t> self_bdim, const at::Tensor& target,
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLoss.cpp:62:huber_loss_batch_rule(const at::Tensor& self, optional<int64_t> self_bdim, const at::Tensor& target,
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLoss.cpp:71:smooth_l1_loss_batch_rule(const at::Tensor& self, optional<int64_t> self_bdim, const at::Tensor& target,
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesConvolution.cpp:20:convolution_batch_rule(const Tensor& lhs, optional<int64_t> lhs_bdim, const Tensor& rhs, optional<int64_t> rhs_bdim, const optional<Tensor>& bias, optional<int64_t> bias_bdim, IntArrayRef stride, c10::SymIntArrayRef padding, IntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, int64_t groups) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesConvolution.cpp:243:convolution_backward_input_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesConvolution.cpp:324:convolution_backward_weight_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesFactory.cpp:65:static std::tuple<Tensor,optional<int64_t>> _new_zeros_with_same_feature_meta_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesFactory.cpp:106:static bool _has_same_storage_numel_batch_rule(const Tensor& a, const Tensor& b) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesPooling.cpp:41:max_pool3d_with_indices_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesPooling.cpp:49:max_pool2d_with_indices_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/PyTorchOperatorHacks.cpp:187:static bool is_fused_kernel_acceptable(const Tensor& input, double p) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:17:std::tuple<Tensor,optional<int64_t>> _binary_pointwise_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:100:void binary_pointwise_inplace_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:127:std::tuple<Tensor,optional<int64_t>> comparison_pointwise_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:149:static std::tuple<Tensor,optional<int64_t>> where_self_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:167:static std::tuple<Tensor, optional<int64_t>> gelu_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:184:static std::tuple<Tensor,optional<int64_t>> masked_select_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:203:static std::tuple<Tensor,optional<int64_t>> masked_select_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:228:static std::tuple<Tensor,optional<int64_t>> cdist_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:273:static void fill__Tensor_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:292:static std::tuple<Tensor, optional<int64_t>> log_sigmoid_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:102:std::vector<Tensor> tensor_split_sections_batching_rule(const Tensor& self, int64_t sections, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:114:std::vector<Tensor> tensor_split_indices_batching_rule(const Tensor& self, IntArrayRef indices, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:126:Tensor& squeeze_dims__batching_rule(Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:168:Tensor& squeeze_dim__batching_rule(Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:172:Tensor& squeeze__batching_rule(Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:209:Tensor& unsqueeze__batching_rule(Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:229:Tensor& transpose__batching_rule(Tensor& self, int64_t dim0, int64_t dim1) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:261:std::vector<Tensor> split_batching_rule(const Tensor& self, int64_t split_size, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:273:std::vector<Tensor> split_with_sizes_batching_rule(const Tensor& self, IntArrayRef split_sizes, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:285:std::vector<Tensor> unbind_batching_rule(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:372:Tensor as_strided_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:526:Tensor cat_batching_rule(const ITensorListRef& tensors, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:593:Tensor block_diag_batching_rule(TensorList tensors) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:621:Tensor stack_batching_rule(TensorList tensors, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:639:Tensor new_empty_strided_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:23:Tensor random_batching_rule(SymIntArrayRef shape, ExtraArgs... extra_args) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:39:Tensor& random_inplace_batching_rule(Tensor& self, ExtraArgs... extra_args) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:64:static Tensor& bernoulli_inplace_Tensor_batching_rule(Tensor& self, const Tensor& p_, c10::optional<Generator> gen) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:114:Tensor randperm_batching_rule(int64_t n, ExtraArgs... extra_args) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:133:Tensor unary_pointwise_random_batch_rule(const Tensor& tensor, ExtraArgs... extra_args) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:161:Tensor tensor_like_random_batch_rule(const Tensor& self, ExtraArgs... extra_args) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:187:static std::tuple<Tensor,Tensor> native_dropout_batching_rule(const Tensor& tensor, double p, c10::optional<bool> train) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:229:static Tensor multinomial_batching_rule(const Tensor& self, const int64_t num_samples, const bool replacement, const c10::optional<Generator> generator) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesActivation.cpp:15:glu_batch_rule(const Tensor& self, optional<int64_t> self_bdim, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesActivation.cpp:30:static std::tuple<Tensor,optional<int64_t>> glu_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:175:std::tuple<Tensor,optional<int64_t>> index_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:486:void index_put__batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:524:void _index_put_impl__batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:618:std::tuple<Tensor,optional<int64_t>> index_put_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:686:std::tuple<Tensor,optional<int64_t>> scatter_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:718:inline std::tuple<Tensor,optional<int64_t>> scatter_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:757:std::tuple<Tensor,optional<int64_t>> scatter_value_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:766:std::tuple<Tensor,optional<int64_t>> scatter_src_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:775:std::tuple<Tensor,optional<int64_t>> scatter_add_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:784:std::tuple<Tensor,optional<int64_t>> scatter_reduce_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:794:std::tuple<Tensor,optional<int64_t>> scatter_value_reduce_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:804:std::tuple<Tensor,optional<int64_t>> gather_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:917:std::tuple<Tensor, optional<int64_t>> diagonal_scatter_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:937:std::tuple<Tensor,optional<int64_t>> index_add_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1008:std::tuple<Tensor,optional<int64_t>> masked_fill_scalar_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1161:void index_fill__int_scalar_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1169:void index_fill__int_tensor_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1177:std::tuple<Tensor,optional<int64_t>> index_fill_int_scalar_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1186:std::tuple<Tensor,optional<int64_t>> index_fill_int_tensor_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/DynamicLibrary.cpp:28:DynamicLibrary::DynamicLibrary(const char* name, const char* alt_name, bool leak_handle_): leak_handle(leak_handle_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/DynamicLibrary.cpp:59:DynamicLibrary::DynamicLibrary(const char* name, const char* alt_name, bool leak_handle_): leak_handle(leak_handle_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/metal/Context.cpp:25:bool is_metal_available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/vulkan/Context.cpp:29:bool is_vulkan_available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ChanelShuffle.cpp:35:Tensor channel_shuffle(const Tensor& self, int64_t groups) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ChanelShuffle.cpp:61:Tensor math_channel_shuffle(const Tensor& self, int64_t groups) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp:16:void cpu_channel_shuffle(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/PixelShuffleKernel.cpp:16:void cpu_pixel_shuffle(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/PixelShuffleKernel.cpp:114:void cpu_pixel_unshuffle(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp:50:Tensor expandScale(const Tensor& t, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NNPACK.cpp:33:bool _nnpack_available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NNPACK.cpp:100:bool _nnpack_available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:99:bool is_fast_path_index_select_scale(const Tensor& src, const Tensor& scale, Tensor& output, index_t padding_idx) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Context.cpp:97:bool available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Shader.cpp:119:ShaderModule::ShaderModule(const VkDevice device, const ShaderInfo& source)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Shader.cpp:136:ShaderModule::ShaderModule(ShaderModule&& other) noexcept
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Shader.cpp:141:ShaderModule::~ShaderModule() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Adapter.cpp:246:DeviceHandle::DeviceHandle(const VkDevice device) : handle_(device) {}
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Adapter.cpp:248:DeviceHandle::DeviceHandle(DeviceHandle&& other) noexcept
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Adapter.cpp:253:DeviceHandle::~DeviceHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Descriptor.cpp:76:VkDescriptorSet DescriptorSet::get_bind_handle() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Descriptor.cpp:133:DescriptorSetPile::DescriptorSetPile(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Command.cpp:344:VkCommandBuffer CommandBuffer::get_submit_handle(const bool final_use) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Convolution.cpp:658:bool available(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Convolution.cpp:716:bool usable(const Tensor& input, const bool quantized) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Mm.cpp:158:bool available(const Tensor& weight, const c10::optional<Tensor>& bias) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Mm.cpp:182:bool usable(const Tensor& input, const IntArrayRef unpacked_weight_sizes) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Tile.cpp:20:Tensor tile(const Tensor& self, const IntArrayRef repeats) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/PixelShuffle.cpp:106:Tensor math_pixel_shuffle(const Tensor& self, int64_t upscale_factor) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/PixelShuffle.cpp:147:Tensor math_pixel_unshuffle(const Tensor& self, int64_t downscale_factor) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/Init.cpp:53:bool available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/Linear.cpp:15:bool available(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/Linear.cpp:40:bool usable(const Tensor& input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/ChannelShuffle.cpp:9:bool use_channel_shuffle(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/ChannelShuffle.cpp:35:Tensor channel_shuffle(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/Shim.cpp:27:bool available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/Convolution.cpp:27:bool available(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/Convolution.cpp:74:bool usable(const Tensor& input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorProperties.cpp:82:bool cudnn_is_acceptable(const TensorBase& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorProperties.cpp:99:bool cudnn_is_acceptable(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:571:Tensor angle(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Dropout.cpp:45:bool is_fused_kernel_acceptable(const Tensor& input, double p) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:197:Tensor NestedTensor_from_padded_and_nested_example(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1570:Tensor from_file(c10::string_view filename, c10::optional<bool> shared, c10::optional<int64_t> size,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1178:at::Tensor convolution_overrideable(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1705:std::tuple<Tensor, Tensor, Tensor> convolution_backward_overrideable(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1716:static Tensor subvariable(const Tensor& var, int dim, int groups, int g) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BatchLinearAlgebraKernel.cpp:71:void apply_reflect_conj_tri_single(scalar_t* self, int64_t n, int64_t stride, bool upper) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1524:Tensor tile(const Tensor& self, IntArrayRef reps){
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp:49:Tensor expandScale(const Tensor& t, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TriangularOps.cpp:43:void apply_triu_tril_single(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cuda/linalg/CusolverDnHandlePool.cpp:9:void createCusolverDnHandle(cusolverDnHandle_t *handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cuda/linalg/CusolverDnHandlePool.cpp:13:void destroyCusolverDnHandle(cusolverDnHandle_t handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cuda/linalg/CusolverDnHandlePool.cpp:31:cusolverDnHandle_t getCurrentCUDASolverDnHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cuda/SpectralOps.cpp:324:double _fft_normalization_scale(int64_t normalization, IntArrayRef sizes, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qsoftmax.cpp:23:bool is_qnnpack_compatible(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/TensorCompare.cpp:48:std::tuple<Tensor, Tensor> sort_quantized_cpu_stable(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Sorting.cpp:693:Tensor quantile(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Sorting.cpp:708:Tensor quantile(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Sorting.cpp:756:Tensor nanquantile(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Sorting.cpp:771:Tensor nanquantile(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Sorting.cpp:991:Tensor argsort_stable(const Tensor & self, bool stable, int64_t dim, bool descending) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1434:Tensor less_equal(const Tensor& self, const Tensor& other) { return self.le(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1437:Tensor less_equal(const Tensor& self, const Scalar& other) { return self.le(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/record_function.cpp:16:CallbackHandle next_unique_callback_handle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/record_function.cpp:21:RecordFunctionHandle next_unique_record_function_handle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:287:c10::optional<float> CPUGeneratorImpl::next_float_normal_sample() {
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:294:c10::optional<double> CPUGeneratorImpl::next_double_normal_sample() {
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:303:void CPUGeneratorImpl::set_next_float_normal_sample(c10::optional<float> randn) {
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:312:void CPUGeneratorImpl::set_next_double_normal_sample(c10::optional<double> randn) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchedTensorImpl.cpp:143:bool inplaceIsVmapCompatible(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/MapAllocator.cpp:36:TORCH_API std::string NewProcessWideShmHandle()
   - /home/haozhe/code/pytorch/aten/src/ATen/MapAllocator.cpp:372:static void CALLBACK WaitForReleaseHandle(PVOID lpParam, BOOLEAN TimerOrWaitFired)
   - /home/haozhe/code/pytorch/aten/src/ATen/SavedTensorHooks.cpp:27:void SavedTensorDefaultHooks::disable(const std::string& message) {
   - /home/haozhe/code/pytorch/aten/src/ATen/SavedTensorHooks.cpp:34:void SavedTensorDefaultHooks::enable() {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/op_registration/op_registration.cpp:12:void build_feature_required_feature_not_available(const char* feature) {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/type_factory.cpp:61:c10::TypePtr DefaultTypeFactory::createNamedTuple(
   - /home/haozhe/code/pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:564:std::string OperatorEntry::dumpComputedTable() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp:175:OperatorHandle::~OperatorHandle() = default;
   - /home/haozhe/code/pytorch/aten/src/ATen/core/Formatting.cpp:161:static void printScale(std::ostream & stream, double scale) {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/boxing/KernelFunction_test.cpp:111:OperatorHandle makeDummyOperatorHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/type.cpp:709:bool Type::is_module() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:223:inline std::vector<c10::IValue> callOpByHandle(
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:5434:void test_tile(
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_quantized_api_test.cpp:96:inline std::vector<c10::IValue> callOpByHandle(
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_quantized_api_test.cpp:119:double rand_double(const double min, const double max) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_quantized_api_test.cpp:157:double produce_random_scale(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:65:Tensor sum_batching_rule(const Tensor& self, OptionalIntArrayRef opt_dims, bool keepdim, optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:96:Tensor binary_pointwise_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:155:Tensor expand_batching_rule(const Tensor& self, IntArrayRef size, bool implicit) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:193:std::vector<Tensor> chunk_batching_rule(const Tensor& self, int64_t chunks, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:201:Tensor clamp_batching_rule(const Tensor& self, const optional<Scalar>& min, const optional<Scalar>& max) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:207:Tensor clamp_min_batching_rule(const Tensor& self, const Scalar& min) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:213:Tensor clamp_max_batching_rule(const Tensor& self, const Scalar& max) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:219:std::vector<Tensor> tensor_split_sections_batching_rule(const Tensor& self, int64_t sections, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:227:std::vector<Tensor> tensor_split_indices_batching_rule(const Tensor& self, IntArrayRef indices, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:235:Tensor unsqueeze_batching_rule(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:247:Tensor& fill_inplace_scalar_batching_rule(Tensor& self, const Scalar& value) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:253:Tensor& fill_inplace_tensor_batching_rule(Tensor& self, const Tensor& value) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:267:Tensor& zero_inplace_batching_rule(Tensor &self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:273:Tensor squeeze_batching_rule(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:294:Tensor squeeze_dim_batching_rule(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:301:Tensor squeeze_dims_batching_rule(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:308:Tensor trace_batching_rule(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:316:Tensor trace_backward_batching_rule(const Tensor& grad, IntArrayRef input_sizes) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:327:Tensor transpose_int_batching_rule(const Tensor& self, int64_t dim0, int64_t dim1) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:344:Tensor permute_batching_rule(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:361:Tensor select_batching_rule(const Tensor& self, int64_t dim, int64_t index) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:372:Tensor select_backward_batching_rule(const Tensor& grad, IntArrayRef input_sizes, int64_t dim, int64_t index) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:380:Tensor slice_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:392:Tensor slice_backward_batching_rule(const Tensor& grad, IntArrayRef input_sizes, int64_t dim, int64_t start, int64_t end, int64_t step) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:400:Tensor diagonal_batching_rule(const Tensor& self, int64_t offset, int64_t dim1, int64_t dim2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:408:Tensor diagonal_backward_batching_rule(const Tensor& grad, IntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:417:Tensor movedim_batching_rule(const Tensor& self, IntArrayRef source, IntArrayRef destination) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:425:Tensor reshape_batching_rule(const Tensor& self, IntArrayRef shape) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:432:std::vector<Tensor> split_batching_rule(const Tensor& self, int64_t split_size, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:440:std::vector<Tensor> split_with_sizes_batching_rule(const Tensor& self, IntArrayRef split_sizes, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:448:std::vector<Tensor> unbind_batching_rule(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:456:Tensor unfold_batching_rule(const Tensor& self, int64_t dim, int64_t size, int64_t step) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:463:Tensor contiguous_batching_rule(const Tensor& self, MemoryFormat memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:472:Tensor view_batching_rule(const Tensor& self, IntArrayRef size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:479:Tensor view_as_complex_batching_rule(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:561:Tensor _reshape_alias_batching_rule(const Tensor& self, IntArrayRef sizes, IntArrayRef strides) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:565:Tensor _new_zeros_with_same_feature_meta_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:582:bool _has_same_storage_numel_batching_rule(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:613:Tensor as_strided_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:759:Tensor pow_scalar_Tensor_batching_rule(const Scalar& other, const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:766:Tensor clone_batching_rule(const Tensor& self, optional<MemoryFormat> memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:808:Tensor mv_batching_rule(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:845:Tensor _make_dual_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:856:Tensor dot_batching_rule(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:892:Tensor bmm_batching_rule(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:903:Tensor mm_batching_rule(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:931:Tensor cat_batching_rule(const ITensorListRef& tensors, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:941:Tensor stack_batching_rule(TensorList tensors, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:959:Tensor to_dtype_layout_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:978:Tensor new_zeros_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:996:Tensor new_empty_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1009:Tensor new_empty_strided_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1069:Tensor comparison_pointwise_batching_rule(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cudnn/Handle.cpp:10:void createCuDNNHandle(cudnnHandle_t *handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cudnn/Handle.cpp:14:void destroyCuDNNHandle(cudnnHandle_t /*handle*/) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cudnn/Handle.cpp:36:cudnnHandle_t getCudnnHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CuSparseHandlePool.cpp:7:void createCusparseHandle(cusparseHandle_t *handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CuSparseHandlePool.cpp:11:void destroyCusparseHandle(cusparseHandle_t handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CuSparseHandlePool.cpp:28:cusparseHandle_t getCurrentCUDASparseHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:21:void createCublasHandle(cublasHandle_t *handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:25:void destroyCublasHandle(cublasHandle_t handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:86:cublasHandle_t getCurrentCUDABlasHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CUDAGraph.cpp:13:MempoolId_t graph_pool_handle() {

  - le_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/ChannelShuffle.cpp:101:Tensor channel_shuffle_quantized_cpu(

124. le.Tensor
  - le
   - /home/haozhe/code/pytorch/aten/src/ATen/Context.cpp:356:bool Context::isXNNPACKAvailable() {
   - /home/haozhe/code/pytorch/aten/src/ATen/miopen/Handle.cpp:9:void createMIOpenHandle(miopenHandle_t *handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/miopen/Handle.cpp:13:void destroyMIOpenHandle(miopenHandle_t handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/miopen/Handle.cpp:35:miopenHandle_t getMiopenHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesNorm.cpp:46:batch_norm_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesNorm.cpp:127:std::tuple<at::Tensor,optional<int64_t>> batch_norm_backward_no_weight_bias_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesNorm.cpp:352:static std::tuple<at::Tensor,optional<int64_t>> group_norm_backward_no_weight_bias_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesNorm.cpp:508:native_layer_norm_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesNorm.cpp:553:static std::tuple<at::Tensor,optional<int64_t>> native_layer_norm_backward_no_weight_bias_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:25:static std::tuple<Tensor, optional<int64_t>> _is_all_true_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:30:static std::tuple<Tensor, optional<int64_t>> _is_any_true_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:106:void boxed_reduction_batch_rule(const c10::OperatorHandle& op, torch::jit::Stack* stack) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:261:static std::tuple<Tensor,optional<int64_t>> _softmax_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:291:static std::tuple<Tensor,optional<int64_t>> _log_softmax_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesReduceOps.cpp:319:static std::tuple<Tensor,optional<int64_t>> searchsorted_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:94:std::tuple<Tensor,optional<int64_t>> unsqueeze_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:105:std::tuple<Tensor,optional<int64_t>> repeat_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:120:std::tuple<Tensor,optional<int64_t>> _unsafe_view_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:141:std::tuple<Tensor,optional<int64_t>> flip_batch_rule(const Tensor& self, optional<int64_t> self_bdim, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:191:std::tuple<Tensor, optional<int64_t>> squeeze_batch_rule(const Tensor& self, optional<int64_t> bdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:226:std::tuple<Tensor, optional<int64_t>> squeeze_dims_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:258:std::tuple<Tensor, optional<int64_t>> squeeze_dim_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:263:std::tuple<Tensor, optional<int64_t>> select_batching_rule(const Tensor& self, optional<int64_t> bdim, int64_t dim, c10::SymInt index) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:274:std::tuple<Tensor, optional<int64_t>> _reshape_alias_batch_rule(const Tensor& self, optional<int64_t> bdim, const c10::SymIntArrayRef shape, const c10::SymIntArrayRef strides) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:285:std::tuple<Tensor, optional<int64_t>> roll_batch_rule(const Tensor& self, optional<int64_t> bdim, SymIntArrayRef shifts, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:311:std::tuple<Tensor, optional<int64_t>> diagonal_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:323:std::tuple<Tensor,optional<int64_t>> diagonal_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:337:std::tuple<Tensor,optional<int64_t>> slice_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:356:transpose_int_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:377:std::tuple<Tensor, optional<int64_t>> permute_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:395:std::tuple<Tensor,optional<int64_t>> select_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:408:std::tuple<Tensor,optional<int64_t>> slice_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:421:std::tuple<Tensor, optional<int64_t>> view_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:433:std::tuple<Tensor,optional<int64_t>> view_copy_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:447:std::tuple<Tensor, optional<int64_t>> expand_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:481:std::tuple<Tensor, optional<int64_t>> unfold_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:498:std::tuple<Tensor, optional<int64_t>> narrow_copy_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:510:std::tuple<std::vector<Tensor>, optional<int64_t>> unsafe_split_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:523:std::tuple<Tensor, optional<int64_t>> movedim_batch_rule(const Tensor& self, optional<int64_t> self_bdim, IntArrayRef source, IntArrayRef destination) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:530:std::tuple<Tensor, optional<int64_t>> diag_embed_batch_rule(const Tensor& self, optional<int64_t> self_bdim, int64_t offset, int64_t dim1, int64_t dim2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:543:std::tuple<Tensor,optional<int64_t>> tril_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:553:std::tuple<Tensor,optional<int64_t>> triu_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesUnaryOps.cpp:14:clone_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesUnaryOps.cpp:52:view_as_complex_batch_rule(const Tensor& self, optional<int64_t> self_bdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesUnaryOps.cpp:63:to_other_batch_rule(const Tensor& self, optional<int64_t> self_bdim,
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:25:std::tuple<Tensor, optional<int64_t>> dot_batch_rule(const Tensor& A, optional<int64_t> A_bdim, const Tensor& B, optional<int64_t> B_bdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:42:static std::tuple<Tensor, optional<int64_t>> tv_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:69:static std::tuple<Tensor, optional<int64_t>> mv_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:82:static std::tuple<Tensor, optional<int64_t>> mm_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:97:static std::tuple<Tensor, optional<int64_t>> bmm_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:169:void _linalg_check_errors_batch_rule(const Tensor& info, optional<int64_t> info_bdim, c10::string_view api_name, bool is_matrix) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:176:householder_product_batch_rule(const Tensor &input, c10::optional<int64_t> input_bdim,
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:283:oneOutput linalg_lu_solve_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:325:oneOutput cholesky_solve_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:340:threeOutputs linalg_lu_factor_ex_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:348:twoOutputs linalg_lu_factor_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:356:oneOutput matrix_exp_batch_rule(const Tensor& self, c10::optional<int64_t> self_bdim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:362:fourOutputs solve_ex_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:404:oneOutput cross_batch_rule(const Tensor& self, c10::optional<int64_t> self_bdim,
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:429:fourOutputs linalg_lstsq_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:455:atol_rtol_tensor_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:484:pinv_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesModules.cpp:23:static std::tuple<Tensor,optional<int64_t>> embedding_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesModules.cpp:54:embedding_dense_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesModules.cpp:113:grid_sample_batch_rule(const Tensor& input, optional<int64_t> input_bdim, const Tensor& grid, optional<int64_t> grid_bdim, ExtraArgs... extra_args) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesModules.cpp:179:grid_sample_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesModules.cpp:200:cudnn_grid_sample_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLoss.cpp:53:mse_loss_batch_rule(const at::Tensor& self, optional<int64_t> self_bdim, const at::Tensor& target,
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLoss.cpp:62:huber_loss_batch_rule(const at::Tensor& self, optional<int64_t> self_bdim, const at::Tensor& target,
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesLoss.cpp:71:smooth_l1_loss_batch_rule(const at::Tensor& self, optional<int64_t> self_bdim, const at::Tensor& target,
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesConvolution.cpp:20:convolution_batch_rule(const Tensor& lhs, optional<int64_t> lhs_bdim, const Tensor& rhs, optional<int64_t> rhs_bdim, const optional<Tensor>& bias, optional<int64_t> bias_bdim, IntArrayRef stride, c10::SymIntArrayRef padding, IntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, int64_t groups) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesConvolution.cpp:243:convolution_backward_input_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesConvolution.cpp:324:convolution_backward_weight_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesFactory.cpp:65:static std::tuple<Tensor,optional<int64_t>> _new_zeros_with_same_feature_meta_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesFactory.cpp:106:static bool _has_same_storage_numel_batch_rule(const Tensor& a, const Tensor& b) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesPooling.cpp:41:max_pool3d_with_indices_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesPooling.cpp:49:max_pool2d_with_indices_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/PyTorchOperatorHacks.cpp:187:static bool is_fused_kernel_acceptable(const Tensor& input, double p) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:17:std::tuple<Tensor,optional<int64_t>> _binary_pointwise_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:100:void binary_pointwise_inplace_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:127:std::tuple<Tensor,optional<int64_t>> comparison_pointwise_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:149:static std::tuple<Tensor,optional<int64_t>> where_self_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:167:static std::tuple<Tensor, optional<int64_t>> gelu_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:184:static std::tuple<Tensor,optional<int64_t>> masked_select_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:203:static std::tuple<Tensor,optional<int64_t>> masked_select_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:228:static std::tuple<Tensor,optional<int64_t>> cdist_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:273:static void fill__Tensor_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:292:static std::tuple<Tensor, optional<int64_t>> log_sigmoid_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:102:std::vector<Tensor> tensor_split_sections_batching_rule(const Tensor& self, int64_t sections, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:114:std::vector<Tensor> tensor_split_indices_batching_rule(const Tensor& self, IntArrayRef indices, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:126:Tensor& squeeze_dims__batching_rule(Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:168:Tensor& squeeze_dim__batching_rule(Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:172:Tensor& squeeze__batching_rule(Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:209:Tensor& unsqueeze__batching_rule(Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:229:Tensor& transpose__batching_rule(Tensor& self, int64_t dim0, int64_t dim1) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:261:std::vector<Tensor> split_batching_rule(const Tensor& self, int64_t split_size, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:273:std::vector<Tensor> split_with_sizes_batching_rule(const Tensor& self, IntArrayRef split_sizes, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:285:std::vector<Tensor> unbind_batching_rule(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:372:Tensor as_strided_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:526:Tensor cat_batching_rule(const ITensorListRef& tensors, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:593:Tensor block_diag_batching_rule(TensorList tensors) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:621:Tensor stack_batching_rule(TensorList tensors, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:639:Tensor new_empty_strided_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:23:Tensor random_batching_rule(SymIntArrayRef shape, ExtraArgs... extra_args) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:39:Tensor& random_inplace_batching_rule(Tensor& self, ExtraArgs... extra_args) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:64:static Tensor& bernoulli_inplace_Tensor_batching_rule(Tensor& self, const Tensor& p_, c10::optional<Generator> gen) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:114:Tensor randperm_batching_rule(int64_t n, ExtraArgs... extra_args) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:133:Tensor unary_pointwise_random_batch_rule(const Tensor& tensor, ExtraArgs... extra_args) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:161:Tensor tensor_like_random_batch_rule(const Tensor& self, ExtraArgs... extra_args) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:187:static std::tuple<Tensor,Tensor> native_dropout_batching_rule(const Tensor& tensor, double p, c10::optional<bool> train) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesRandomness.cpp:229:static Tensor multinomial_batching_rule(const Tensor& self, const int64_t num_samples, const bool replacement, const c10::optional<Generator> generator) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesActivation.cpp:15:glu_batch_rule(const Tensor& self, optional<int64_t> self_bdim, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesActivation.cpp:30:static std::tuple<Tensor,optional<int64_t>> glu_backward_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:175:std::tuple<Tensor,optional<int64_t>> index_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:486:void index_put__batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:524:void _index_put_impl__batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:618:std::tuple<Tensor,optional<int64_t>> index_put_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:686:std::tuple<Tensor,optional<int64_t>> scatter_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:718:inline std::tuple<Tensor,optional<int64_t>> scatter_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:757:std::tuple<Tensor,optional<int64_t>> scatter_value_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:766:std::tuple<Tensor,optional<int64_t>> scatter_src_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:775:std::tuple<Tensor,optional<int64_t>> scatter_add_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:784:std::tuple<Tensor,optional<int64_t>> scatter_reduce_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:794:std::tuple<Tensor,optional<int64_t>> scatter_value_reduce_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:804:std::tuple<Tensor,optional<int64_t>> gather_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:917:std::tuple<Tensor, optional<int64_t>> diagonal_scatter_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:937:std::tuple<Tensor,optional<int64_t>> index_add_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1008:std::tuple<Tensor,optional<int64_t>> masked_fill_scalar_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1161:void index_fill__int_scalar_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1169:void index_fill__int_tensor_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1177:std::tuple<Tensor,optional<int64_t>> index_fill_int_scalar_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1186:std::tuple<Tensor,optional<int64_t>> index_fill_int_tensor_batch_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/DynamicLibrary.cpp:28:DynamicLibrary::DynamicLibrary(const char* name, const char* alt_name, bool leak_handle_): leak_handle(leak_handle_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/DynamicLibrary.cpp:59:DynamicLibrary::DynamicLibrary(const char* name, const char* alt_name, bool leak_handle_): leak_handle(leak_handle_) {
   - /home/haozhe/code/pytorch/aten/src/ATen/metal/Context.cpp:25:bool is_metal_available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/vulkan/Context.cpp:29:bool is_vulkan_available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ChanelShuffle.cpp:35:Tensor channel_shuffle(const Tensor& self, int64_t groups) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ChanelShuffle.cpp:61:Tensor math_channel_shuffle(const Tensor& self, int64_t groups) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp:16:void cpu_channel_shuffle(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/PixelShuffleKernel.cpp:16:void cpu_pixel_shuffle(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/PixelShuffleKernel.cpp:114:void cpu_pixel_unshuffle(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp:50:Tensor expandScale(const Tensor& t, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NNPACK.cpp:33:bool _nnpack_available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NNPACK.cpp:100:bool _nnpack_available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:99:bool is_fast_path_index_select_scale(const Tensor& src, const Tensor& scale, Tensor& output, index_t padding_idx) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Context.cpp:97:bool available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Shader.cpp:119:ShaderModule::ShaderModule(const VkDevice device, const ShaderInfo& source)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Shader.cpp:136:ShaderModule::ShaderModule(ShaderModule&& other) noexcept
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Shader.cpp:141:ShaderModule::~ShaderModule() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Adapter.cpp:246:DeviceHandle::DeviceHandle(const VkDevice device) : handle_(device) {}
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Adapter.cpp:248:DeviceHandle::DeviceHandle(DeviceHandle&& other) noexcept
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Adapter.cpp:253:DeviceHandle::~DeviceHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Descriptor.cpp:76:VkDescriptorSet DescriptorSet::get_bind_handle() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Descriptor.cpp:133:DescriptorSetPile::DescriptorSetPile(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/api/Command.cpp:344:VkCommandBuffer CommandBuffer::get_submit_handle(const bool final_use) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Convolution.cpp:658:bool available(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Convolution.cpp:716:bool usable(const Tensor& input, const bool quantized) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Mm.cpp:158:bool available(const Tensor& weight, const c10::optional<Tensor>& bias) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Mm.cpp:182:bool usable(const Tensor& input, const IntArrayRef unpacked_weight_sizes) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Tile.cpp:20:Tensor tile(const Tensor& self, const IntArrayRef repeats) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/PixelShuffle.cpp:106:Tensor math_pixel_shuffle(const Tensor& self, int64_t upscale_factor) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/PixelShuffle.cpp:147:Tensor math_pixel_unshuffle(const Tensor& self, int64_t downscale_factor) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/Init.cpp:53:bool available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/Linear.cpp:15:bool available(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/Linear.cpp:40:bool usable(const Tensor& input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/ChannelShuffle.cpp:9:bool use_channel_shuffle(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/ChannelShuffle.cpp:35:Tensor channel_shuffle(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/Shim.cpp:27:bool available() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/Convolution.cpp:27:bool available(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/xnnpack/Convolution.cpp:74:bool usable(const Tensor& input) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorProperties.cpp:82:bool cudnn_is_acceptable(const TensorBase& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorProperties.cpp:99:bool cudnn_is_acceptable(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:571:Tensor angle(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Dropout.cpp:45:bool is_fused_kernel_acceptable(const Tensor& input, double p) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/nested/NestedTensorMath.cpp:197:Tensor NestedTensor_from_padded_and_nested_example(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorFactories.cpp:1570:Tensor from_file(c10::string_view filename, c10::optional<bool> shared, c10::optional<int64_t> size,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1178:at::Tensor convolution_overrideable(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1705:std::tuple<Tensor, Tensor, Tensor> convolution_backward_overrideable(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Convolution.cpp:1716:static Tensor subvariable(const Tensor& var, int dim, int groups, int g) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BatchLinearAlgebraKernel.cpp:71:void apply_reflect_conj_tri_single(scalar_t* self, int64_t n, int64_t stride, bool upper) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1524:Tensor tile(const Tensor& self, IntArrayRef reps){
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp:49:Tensor expandScale(const Tensor& t, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TriangularOps.cpp:43:void apply_triu_tril_single(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cuda/linalg/CusolverDnHandlePool.cpp:9:void createCusolverDnHandle(cusolverDnHandle_t *handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cuda/linalg/CusolverDnHandlePool.cpp:13:void destroyCusolverDnHandle(cusolverDnHandle_t handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cuda/linalg/CusolverDnHandlePool.cpp:31:cusolverDnHandle_t getCurrentCUDASolverDnHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cuda/SpectralOps.cpp:324:double _fft_normalization_scale(int64_t normalization, IntArrayRef sizes, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qsoftmax.cpp:23:bool is_qnnpack_compatible(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/TensorCompare.cpp:48:std::tuple<Tensor, Tensor> sort_quantized_cpu_stable(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Sorting.cpp:693:Tensor quantile(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Sorting.cpp:708:Tensor quantile(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Sorting.cpp:756:Tensor nanquantile(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Sorting.cpp:771:Tensor nanquantile(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Sorting.cpp:991:Tensor argsort_stable(const Tensor & self, bool stable, int64_t dim, bool descending) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1434:Tensor less_equal(const Tensor& self, const Tensor& other) { return self.le(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1437:Tensor less_equal(const Tensor& self, const Scalar& other) { return self.le(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/record_function.cpp:16:CallbackHandle next_unique_callback_handle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/record_function.cpp:21:RecordFunctionHandle next_unique_record_function_handle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:287:c10::optional<float> CPUGeneratorImpl::next_float_normal_sample() {
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:294:c10::optional<double> CPUGeneratorImpl::next_double_normal_sample() {
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:303:void CPUGeneratorImpl::set_next_float_normal_sample(c10::optional<float> randn) {
   - /home/haozhe/code/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:312:void CPUGeneratorImpl::set_next_double_normal_sample(c10::optional<double> randn) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchedTensorImpl.cpp:143:bool inplaceIsVmapCompatible(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/MapAllocator.cpp:36:TORCH_API std::string NewProcessWideShmHandle()
   - /home/haozhe/code/pytorch/aten/src/ATen/MapAllocator.cpp:372:static void CALLBACK WaitForReleaseHandle(PVOID lpParam, BOOLEAN TimerOrWaitFired)
   - /home/haozhe/code/pytorch/aten/src/ATen/SavedTensorHooks.cpp:27:void SavedTensorDefaultHooks::disable(const std::string& message) {
   - /home/haozhe/code/pytorch/aten/src/ATen/SavedTensorHooks.cpp:34:void SavedTensorDefaultHooks::enable() {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/op_registration/op_registration.cpp:12:void build_feature_required_feature_not_available(const char* feature) {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/type_factory.cpp:61:c10::TypePtr DefaultTypeFactory::createNamedTuple(
   - /home/haozhe/code/pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:564:std::string OperatorEntry::dumpComputedTable() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp:175:OperatorHandle::~OperatorHandle() = default;
   - /home/haozhe/code/pytorch/aten/src/ATen/core/Formatting.cpp:161:static void printScale(std::ostream & stream, double scale) {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/boxing/KernelFunction_test.cpp:111:OperatorHandle makeDummyOperatorHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/type.cpp:709:bool Type::is_module() const {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:223:inline std::vector<c10::IValue> callOpByHandle(
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_api_test.cpp:5434:void test_tile(
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_quantized_api_test.cpp:96:inline std::vector<c10::IValue> callOpByHandle(
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_quantized_api_test.cpp:119:double rand_double(const double min, const double max) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/vulkan_quantized_api_test.cpp:157:double produce_random_scale(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:65:Tensor sum_batching_rule(const Tensor& self, OptionalIntArrayRef opt_dims, bool keepdim, optional<ScalarType> dtype) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:96:Tensor binary_pointwise_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:155:Tensor expand_batching_rule(const Tensor& self, IntArrayRef size, bool implicit) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:193:std::vector<Tensor> chunk_batching_rule(const Tensor& self, int64_t chunks, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:201:Tensor clamp_batching_rule(const Tensor& self, const optional<Scalar>& min, const optional<Scalar>& max) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:207:Tensor clamp_min_batching_rule(const Tensor& self, const Scalar& min) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:213:Tensor clamp_max_batching_rule(const Tensor& self, const Scalar& max) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:219:std::vector<Tensor> tensor_split_sections_batching_rule(const Tensor& self, int64_t sections, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:227:std::vector<Tensor> tensor_split_indices_batching_rule(const Tensor& self, IntArrayRef indices, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:235:Tensor unsqueeze_batching_rule(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:247:Tensor& fill_inplace_scalar_batching_rule(Tensor& self, const Scalar& value) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:253:Tensor& fill_inplace_tensor_batching_rule(Tensor& self, const Tensor& value) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:267:Tensor& zero_inplace_batching_rule(Tensor &self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:273:Tensor squeeze_batching_rule(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:294:Tensor squeeze_dim_batching_rule(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:301:Tensor squeeze_dims_batching_rule(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:308:Tensor trace_batching_rule(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:316:Tensor trace_backward_batching_rule(const Tensor& grad, IntArrayRef input_sizes) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:327:Tensor transpose_int_batching_rule(const Tensor& self, int64_t dim0, int64_t dim1) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:344:Tensor permute_batching_rule(const Tensor& self, IntArrayRef dims) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:361:Tensor select_batching_rule(const Tensor& self, int64_t dim, int64_t index) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:372:Tensor select_backward_batching_rule(const Tensor& grad, IntArrayRef input_sizes, int64_t dim, int64_t index) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:380:Tensor slice_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:392:Tensor slice_backward_batching_rule(const Tensor& grad, IntArrayRef input_sizes, int64_t dim, int64_t start, int64_t end, int64_t step) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:400:Tensor diagonal_batching_rule(const Tensor& self, int64_t offset, int64_t dim1, int64_t dim2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:408:Tensor diagonal_backward_batching_rule(const Tensor& grad, IntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:417:Tensor movedim_batching_rule(const Tensor& self, IntArrayRef source, IntArrayRef destination) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:425:Tensor reshape_batching_rule(const Tensor& self, IntArrayRef shape) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:432:std::vector<Tensor> split_batching_rule(const Tensor& self, int64_t split_size, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:440:std::vector<Tensor> split_with_sizes_batching_rule(const Tensor& self, IntArrayRef split_sizes, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:448:std::vector<Tensor> unbind_batching_rule(const Tensor& self, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:456:Tensor unfold_batching_rule(const Tensor& self, int64_t dim, int64_t size, int64_t step) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:463:Tensor contiguous_batching_rule(const Tensor& self, MemoryFormat memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:472:Tensor view_batching_rule(const Tensor& self, IntArrayRef size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:479:Tensor view_as_complex_batching_rule(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:561:Tensor _reshape_alias_batching_rule(const Tensor& self, IntArrayRef sizes, IntArrayRef strides) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:565:Tensor _new_zeros_with_same_feature_meta_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:582:bool _has_same_storage_numel_batching_rule(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:613:Tensor as_strided_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:759:Tensor pow_scalar_Tensor_batching_rule(const Scalar& other, const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:766:Tensor clone_batching_rule(const Tensor& self, optional<MemoryFormat> memory_format) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:808:Tensor mv_batching_rule(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:845:Tensor _make_dual_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:856:Tensor dot_batching_rule(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:892:Tensor bmm_batching_rule(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:903:Tensor mm_batching_rule(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:931:Tensor cat_batching_rule(const ITensorListRef& tensors, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:941:Tensor stack_batching_rule(TensorList tensors, int64_t dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:959:Tensor to_dtype_layout_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:978:Tensor new_zeros_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:996:Tensor new_empty_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1009:Tensor new_empty_strided_batching_rule(
   - /home/haozhe/code/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1069:Tensor comparison_pointwise_batching_rule(const Tensor& self, const Tensor& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cudnn/Handle.cpp:10:void createCuDNNHandle(cudnnHandle_t *handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cudnn/Handle.cpp:14:void destroyCuDNNHandle(cudnnHandle_t /*handle*/) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cudnn/Handle.cpp:36:cudnnHandle_t getCudnnHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CuSparseHandlePool.cpp:7:void createCusparseHandle(cusparseHandle_t *handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CuSparseHandlePool.cpp:11:void destroyCusparseHandle(cusparseHandle_t handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CuSparseHandlePool.cpp:28:cusparseHandle_t getCurrentCUDASparseHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:21:void createCublasHandle(cublasHandle_t *handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:25:void destroyCublasHandle(cublasHandle_t handle) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:86:cublasHandle_t getCurrentCUDABlasHandle() {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/CUDAGraph.cpp:13:MempoolId_t graph_pool_handle() {

  - le_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/ChannelShuffle.cpp:101:Tensor channel_shuffle_quantized_cpu(

125. gt.Scalar
  - gt
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1442:Tensor greater(const Tensor& self, const Tensor& other) { return self.gt(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1445:Tensor greater(const Tensor& self, const Scalar& other) { return self.gt(other); }

  - gt_quantized_cpu(QuantizedCPU)
126. gt.Tensor
  - gt
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1442:Tensor greater(const Tensor& self, const Tensor& other) { return self.gt(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1445:Tensor greater(const Tensor& self, const Scalar& other) { return self.gt(other); }

  - gt_quantized_cpu(QuantizedCPU)
127. lt.Scalar
  - lt
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkl/SparseBlasImpl.cpp:171:void addmm_dense_result(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkl/SparseBlasImpl.cpp:241:void addmm_sparse_input_dense_result(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkl/SparseBlasImpl.cpp:293:void addmm_sparse_result(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/DistributionKernels.cpp:40:void bernoulli_scalar_kernel_default(const TensorBase &self, double p, c10::optional<Generator> gen) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/DistributionKernels.cpp:101:static void exponential_kernel_default(TensorIteratorBase& iter, double lambda, c10::optional<Generator> gen) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TestOps.cpp:66:Tensor _test_string_default(const Tensor& dummy, c10::string_view a, c10::string_view b) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseMatMul.cpp:92:void _csr_matmult(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp:90:bool is_coalesced_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp:123:Tensor indices_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp:134:Tensor values_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:695:Tensor crow_indices_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:699:Tensor col_indices_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:703:Tensor ccol_indices_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:707:Tensor row_indices_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Memory.cpp:27:bool is_pinned_default(const Tensor& self, c10::optional<Device> device) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp:127:void TensorIteratorBase::foreach_reduced_elt(loop_subiter_t loop, bool parallelize) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1216:void inline set_result(Tensor& result, accscalar_t sum)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1426:Tensor less(const Tensor& self, const Tensor& other) { return self.lt(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1429:Tensor less(const Tensor& self, const Scalar& other) { return self.lt(other); }

  - lt_quantized_cpu(QuantizedCPU)
128. lt.Tensor
  - lt
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkl/SparseBlasImpl.cpp:171:void addmm_dense_result(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkl/SparseBlasImpl.cpp:241:void addmm_sparse_input_dense_result(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkl/SparseBlasImpl.cpp:293:void addmm_sparse_result(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/DistributionKernels.cpp:40:void bernoulli_scalar_kernel_default(const TensorBase &self, double p, c10::optional<Generator> gen) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cpu/DistributionKernels.cpp:101:static void exponential_kernel_default(TensorIteratorBase& iter, double lambda, c10::optional<Generator> gen) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TestOps.cpp:66:Tensor _test_string_default(const Tensor& dummy, c10::string_view a, c10::string_view b) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseMatMul.cpp:92:void _csr_matmult(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp:90:bool is_coalesced_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp:123:Tensor indices_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensor.cpp:134:Tensor values_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:695:Tensor crow_indices_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:699:Tensor col_indices_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:703:Tensor ccol_indices_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseCsrTensor.cpp:707:Tensor row_indices_default(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Memory.cpp:27:bool is_pinned_default(const Tensor& self, c10::optional<Device> device) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorIteratorReduce.cpp:127:void TensorIteratorBase::foreach_reduced_elt(loop_subiter_t loop, bool parallelize) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReduceOps.cpp:1216:void inline set_result(Tensor& result, accscalar_t sum)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1426:Tensor less(const Tensor& self, const Tensor& other) { return self.lt(other); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1429:Tensor less(const Tensor& self, const Scalar& other) { return self.lt(other); }

  - lt_quantized_cpu(QuantizedCPU)
129. index_select
  - index_select
   - /home/haozhe/code/pytorch/aten/src/ATen/native/EmbeddingBag.cpp:89:bool is_fast_path_index_select(const Tensor& src, Tensor& output, index_t padding_idx) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:369:Tensor index_select(const Tensor& self, Dimname dim, const Tensor& index) {

  - index_select_cpu_(CPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1378:Tensor index_select_cpu_(const Tensor & self, int64_t dim, const Tensor & index) {

  - index_select_quantized_cpu_(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1383:Tensor index_select_quantized_cpu_(const Tensor & self, int64_t dim, const Tensor & index) {

  - index_select_cuda(CUDA)
  - index_select_quantized_cuda(QuantizedCUDA)
  - index_select_sparse_cpu(SparseCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1819:Tensor index_select_sparse_cpu(const Tensor& self, int64_t dim, const Tensor& index) {

  - index_select_sparse_cuda(SparseCUDA)
  - index_select_mps(MPS)
130. nonzero
  - nonzero
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:2222:Tensor count_nonzero(const Tensor& self, c10::optional<int64_t> dim) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/prim_native_functions.cpp:15:bool is_nonzero(const Tensor& self) {

  - nonzero_cpu(CPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:2192:Tensor count_nonzero_cpu(const Tensor& self, IntArrayRef dims){
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:2341:Tensor nonzero_cpu(const Tensor& self) {

  - nonzero_cuda(CUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:2188:Tensor count_nonzero_cuda(const Tensor& self, IntArrayRef dims){

  - nonzero_mps(MPS)
131. gather
  - gather
   - /home/haozhe/code/pytorch/aten/src/ATen/native/NamedTensor.cpp:333:Tensor gather(const Tensor& self, Dimname dim, const Tensor& index, bool sparse_grad) {

132. svd
  - svd
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BatchLinearAlgebraKernel.cpp:1025:static void apply_svd(const Tensor& A,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:3207:std::tuple<Tensor, Tensor, Tensor> linalg_svd(const Tensor& A, bool full_matrices,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:3266:std::tuple<Tensor, Tensor, Tensor> svd(const Tensor& self, bool some, bool compute_uv) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp:501:inline static void apply_svd_cusolver_gesvd(const Tensor& A, const Tensor& U, const Tensor& S, const Tensor& V,
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp:573:inline static void svd_cusolver_gesvd(const Tensor& A, const Tensor& U, const Tensor& S, const Tensor& V,

133. lgamma
  - lgamma
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:889:Tensor mvlgamma(const Tensor& self, int64_t p) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:980:Tensor special_gammaln(const Tensor& self) { return self.lgamma(); }

134. digamma
  - digamma
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:694:Tensor special_psi(const Tensor& self) { return self.digamma(); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:697:Tensor special_digamma(const Tensor& self) { return self.digamma(); }

135. sign
  - sign
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:872:Tensor copysign(const Tensor& self, const Scalar& other) {

  - sign_sparse(SparseCPU, SparseCUDA)
  - sign_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
136. signbit
  - signbit


  - signbit_sparse(SparseCPU, SparseCUDA)
  - signbit_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
137. atan2
  - atan2


138. fmod.Tensor
  - fmod
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1534:Tensor fmod(const Tensor& self, const Scalar& other) {

139. hypot
  - hypot


140. igamma
  - igamma
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:694:Tensor special_psi(const Tensor& self) { return self.digamma(); }
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:697:Tensor special_digamma(const Tensor& self) { return self.digamma(); }

141. igammac
  - igammac


142. nextafter
  - nextafter


143. remainder.Tensor
  - remainder
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1169:Tensor remainder(const Tensor& self, const Scalar& other) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/BinaryOps.cpp:1184:Tensor remainder(const Scalar& self, const Tensor& other) {

144. fmin
  - fmin


145. fmax
  - fmax


146. maximum
  - maximum


147. minimum
  - minimum


148. topk
  - topk
   - /home/haozhe/code/pytorch/aten/src/ATen/native/cuda/TensorTopK.cpp:36:bool disable_sort_for_topk();

  - topk_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/Sorting.cpp:45:std::tuple<Tensor, Tensor> topk_quantized_cpu(

149. pow.Tensor_Tensor
  - pow
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:774:Tensor square(const Tensor& self) { return at::pow(self, 2); }
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:116:T typed_pow(T base, T exp) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:121:T typed_pow(T base, T exp) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:854:pow(const complex<_Tp>& __x, const complex<_Tp>& __y)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:862:pow(const complex<_Tp>& __x, const complex<_Up>& __y)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:875:pow(const complex<_Tp>& __x, const _Up& __y)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:888:pow(const _Tp& __x, const complex<_Up>& __y)

150. pow.Tensor_Scalar
  - pow
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UnaryOps.cpp:774:Tensor square(const Tensor& self) { return at::pow(self, 2); }
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:116:T typed_pow(T base, T exp) {
   - /home/haozhe/code/pytorch/aten/src/ATen/test/pow_test.cpp:121:T typed_pow(T base, T exp) {
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:854:pow(const complex<_Tp>& __x, const complex<_Tp>& __y)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:862:pow(const complex<_Tp>& __x, const complex<_Up>& __y)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:875:pow(const complex<_Tp>& __x, const _Up& __y)
   - /home/haozhe/code/pytorch/aten/src/ATen/cuda/llvm_complex.cpp:888:pow(const _Tp& __x, const complex<_Up>& __y)

  - pow_sparse_scalar(SparseCPU, SparseCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:185:SparseTensor pow_sparse_scalar(const SparseTensor& t, const Scalar& value) {

151. alias
  - alias
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Shape.cpp:49:Tensor _reshape_alias(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1671:Tensor _reshape_alias(const Tensor& self, IntArrayRef sizes, IntArrayRef strides) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3653:Tensor alias(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorConversions.cpp:361:bool to_will_alias(
   - /home/haozhe/code/pytorch/aten/src/ATen/core/function_schema.cpp:147:bool FunctionSchema::may_alias(const SchemaArgument& lhs, const SchemaArgument& rhs) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/function_schema.cpp:177:bool FunctionSchema::may_contain_alias(const SchemaArgument& lhs, const SchemaArgument& rhs, bool bidirectional) const {

  - alias(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Shape.cpp:49:Tensor _reshape_alias(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:1671:Tensor _reshape_alias(const Tensor& self, IntArrayRef sizes, IntArrayRef strides) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorShape.cpp:3653:Tensor alias(const Tensor& self) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorConversions.cpp:361:bool to_will_alias(
   - /home/haozhe/code/pytorch/aten/src/ATen/core/function_schema.cpp:147:bool FunctionSchema::may_alias(const SchemaArgument& lhs, const SchemaArgument& rhs) const {
   - /home/haozhe/code/pytorch/aten/src/ATen/core/function_schema.cpp:177:bool FunctionSchema::may_contain_alias(const SchemaArgument& lhs, const SchemaArgument& rhs, bool bidirectional) const {

152. hardtanh
  - hardtanh
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Activation.cpp:429:Tensor hardtanh(const Tensor& self, const Scalar& min, const Scalar& max) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:236:Tensor hardtanh(const Tensor& self, const Scalar& min, const Scalar& max) {

  - hardtanh(CPU, CUDA, MPS)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/Activation.cpp:429:Tensor hardtanh(const Tensor& self, const Scalar& min, const Scalar& max) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:236:Tensor hardtanh(const Tensor& self, const Scalar& min, const Scalar& max) {

  - hardtanh_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qclamp.cpp:141:Tensor hardtanh_quantized_cpu(

153. leaky_relu
  - leaky_relu
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Clamp.cpp:432:Tensor leaky_relu(const Tensor& self_arg, const Scalar& negative_slope) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:885:at::Tensor PackedLinearWeightsOnednn:: apply_leaky_relu(

  - leaky_relu_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/qrelu.cpp:138:Tensor leaky_relu_quantized_cpu(const Tensor& self, const Scalar& negval) {

154. _adaptive_avg_pool2d
  - _adaptive_avg_pool2d
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:96:Tensor mkldnn_adaptive_avg_pool2d(Tensor const& input, IntArrayRef output_size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:487:Tensor mkldnn_adaptive_avg_pool2d(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/AdaptiveAveragePooling.cpp:254:Tensor q_adaptive_avg_pool2d(const Tensor& input, IntArrayRef output_size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/AdaptiveAveragePooling.cpp:266:Tensor qnnpack_adaptive_avg_pool2d(

  - adaptive_avg_pool2d_cpu(CPU)


  - adaptive_avg_pool2d_cuda(CUDA)
  - adaptive_avg_pool2d_mps(MPS)
  - adaptive_avg_pool2d_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/AdaptiveAveragePooling.cpp:314:Tensor adaptive_avg_pool2d_quantized_cpu(

  - adaptive_avg_pool2d_quantized_cuda(QuantizedCUDA)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cudnn/Pooling.cpp:51:Tensor adaptive_avg_pool2d_quantized_cuda(

155. _adaptive_avg_pool2d_backward
  - _adaptive_avg_pool2d_backward
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:184:Tensor mkldnn_adaptive_avg_pool2d_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:642:Tensor mkldnn_adaptive_avg_pool2d_backward(

  - adaptive_avg_pool2d_backward_cpu(CPU)


  - adaptive_avg_pool2d_backward_cuda(CUDA)
  - adaptive_avg_pool2d_backward_mps(MPS)
156. avg_pool2d
  - avg_pool2d
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Pool.cpp:13:Tensor adaptive_avg_pool2d(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Pool.cpp:222:Tensor avg_pool2d(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:52:Tensor mkldnn_avg_pool2d(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:96:Tensor mkldnn_adaptive_avg_pool2d(Tensor const& input, IntArrayRef output_size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:414:Tensor mkldnn_avg_pool2d(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:487:Tensor mkldnn_adaptive_avg_pool2d(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/AveragePool2d.cpp:182:Tensor q_avg_pool2d(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/AveragePool2d.cpp:263:Tensor qnnpack_avg_pool2d(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/AdaptiveAveragePooling.cpp:254:Tensor q_adaptive_avg_pool2d(const Tensor& input, IntArrayRef output_size) {
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/AdaptiveAveragePooling.cpp:266:Tensor qnnpack_adaptive_avg_pool2d(

  - mkldnn_avg_pool2d(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:52:Tensor mkldnn_avg_pool2d(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:414:Tensor mkldnn_avg_pool2d(

  - avg_pool2d_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/AveragePool2d.cpp:363:Tensor avg_pool2d_quantized_cpu(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/quantized/cpu/AdaptiveAveragePooling.cpp:314:Tensor adaptive_avg_pool2d_quantized_cpu(

157. avg_pool2d_backward
  - avg_pool2d_backward
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:148:Tensor mkldnn_avg_pool2d_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:184:Tensor mkldnn_adaptive_avg_pool2d_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:574:Tensor mkldnn_avg_pool2d_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:642:Tensor mkldnn_adaptive_avg_pool2d_backward(

  - mkldnn_avg_pool2d_backward(MkldnnCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:148:Tensor mkldnn_avg_pool2d_backward(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/mkldnn/Pooling.cpp:574:Tensor mkldnn_avg_pool2d_backward(

158. max_pool2d_with_indices
  - max_pool2d_with_indices


159. max_pool2d_with_indices_backward
  - max_pool2d_with_indices_backward
160. max_pool3d_with_indices
  - max_pool3d_with_indices


  - max_pool3d_with_indices_cpu(CPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/DilatedMaxPool3d.cpp:505:std::tuple<Tensor, Tensor> max_pool3d_with_indices_cpu(

  - max_pool3d_with_indices_cuda(CUDA)
161. reflection_pad2d
  - reflection_pad2d
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Padding.cpp:97:Tensor reflection_pad2d(const Tensor& self_arg, IntArrayRef padding) {

  - reflection_pad2d_cpu(CPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReflectionPad.cpp:340:Tensor reflection_pad2d_cpu(const Tensor& input, IntArrayRef padding) {

  - reflection_pad2d_quantized_cpu(QuantizedCPU)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/ReflectionPad.cpp:346:Tensor reflection_pad2d_quantized_cpu(const Tensor& input, IntArrayRef padding) {

  - reflection_pad2d_cuda(CUDA)
  - reflection_pad2d_mps(MPS)
162. replication_pad2d
  - replication_pad2d
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Padding.cpp:101:Tensor replication_pad2d(const Tensor& self_arg, IntArrayRef padding) {

163. replication_pad3d
  - replication_pad3d
164. upsample_bilinear2d.vec
  - upsample_bilinear2d
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Upsample.cpp:96:Tensor upsample_bilinear2d(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UpSampleBilinear2d.cpp:161:Tensor upsample_bilinear2d(

165. upsample_nearest2d.vec
  - upsample_nearest2d
   - /home/haozhe/code/pytorch/aten/src/ATen/native/vulkan/ops/Upsample.cpp:12:Tensor upsample_nearest2d(
   - /home/haozhe/code/pytorch/aten/src/ATen/native/UpSampleNearest2d.cpp:150:Tensor upsample_nearest2d(

166. col2im
  - col2im
167. isfinite
  - isfinite
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:389:Tensor isfinite(const Tensor& self) {

168. isinf
  - isinf
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:372:Tensor isinf(const Tensor &self) {

  - isinf(CompositeExplicitAutograd)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/TensorCompare.cpp:372:Tensor isinf(const Tensor &self) {

  - isinf_sparse(SparseCPU, SparseCUDA)
  - isinf_sparse_meta(SparseMeta)
   - /home/haozhe/code/pytorch/aten/src/ATen/native/sparse/SparseUnaryOps.cpp:205:Tensor isinf_sparse_meta(const Tensor& self) {

  - isinf_sparse_csr(SparseCsrCPU, SparseCsrCUDA)
